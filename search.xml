<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[记一次jvm调优——附带linux下监控JVM常用指令]]></title>
    <url>%2F2019%2F03%2F14%2FLinux%E4%B8%8BJVM%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[Linux下JVM监控——记一次调优 测试版本时发现在项目运行中，tomcat老是莫名其妙的就挂掉半天得不到响应，排查掉代码出现问题的可能后，就想到了JVM虚拟机上面去，用-gccause指令查看了下发现20分钟发生了1000多次full gc，full gc是老年代发生的gc，正常情况下也就10到20次就差不多了，这么频繁的full gc肯定会影响服务器的性能啊，然后思考full gc发生的原因，full gc是老年代的gc，对象被丢到老年代会有两种机制，一种是超过当前年轻代可放置空间了，一种是超过对象大小的设置阀值而被当成大对象直接丢到老年代，于是用-gccause指令再次查看survivor区的占比，发现survivor区常年占比百分之80左右，然后就猜测可能是年轻代放不下了而被丢进的老年代，所以用-gccapacity观察了下新生代和老年代分配的堆内存空间，发现总和只有1G，后续把堆内存分配空间给到了3G问题就解决了。 后续：对于JVM来说大对象是灾难，在程序中应尽量避免，比这个还遭的是一堆朝生夕死的大对象，在写业务时应尽量避免产生这样的大对象。 JDK内置工具使用 jps(Java Virtual Machine Process Status Tool)​ 查看所有的jvm进程，包括进程ID，进程启动的路径等等。 jstack(Java Stack Trace)​ ① 观察jvm中当前所有线程的运行情况和线程当前状态。​ ② 系统崩溃了？如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。​ ③ 系统hung住了？jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstat(Java Virtual Machine Statistics Monitoring Tool)​ ① jstat利用JVM内建的指令对Java应用程序的资源和性能进行实时的命令行的监控，包括了对进程的classloader，compiler，gc情况；​ ②监视VM内存内的各种堆和非堆的大小及其内存使用量，以及加载类的数量。 jmap(Java Memory Map)​ 监视进程运行中的jvm物理内存的占用情况，该进程内存内，所有对象的情况，例如产生了哪些对象，对象数量； ​jinfo(Java Configuration Info)观察进程运行环境参数，包括Java System属性和JVM命令行参数 具体命令使用：jstatgeneralOption 123-help 显示帮助信息。-version 显示版本信息-options 显示统计选项列表。 outputOptions 12345678910111213#参数： -class：统计类装载器的行为 -compiler：统计HotSpot Just-in-Time编译器的行为 -gc：统计堆各个分区的使用情况 -gccapacity：统计新生区，老年区，permanent区的heap容量情况 -gccause：统计最后一次gc和当前gc的原因 -gcnew：统计gc时，新生代的情况 -gcnewcapacity：统计新生代大小和空间 -gcold：统计老年代和永久代的行为 -gcoldcapacity：统计老年代大小 -gcpermcapacity：统计永久代大小 -gcutil：统计gc时，heap情况 -printcompilation：HotSpot编译方法统计 -class：12345678910111213141516#每隔1秒监控一次，一共做10次 jstat -class 17970 1000 10##########################################[root@lq225 conf]# jstat -class 2058 1000 10Loaded Bytes Unloaded Bytes Time 1697 3349.5 0 0.0 1.79 1697 3349.5 0 0.0 1.79 1697 3349.5 0 0.0 1.79 1697 3349.5 0 0.0 1.79 ...................................................######################## 术语分隔符 #########################Loaded 类加载数量#Bytes 加载的大小（k） #Unloaded 类卸载的数量 #Bytes 卸载的大小（k） #Time 时间花费在执行类加载和卸载操作 -compiler1234567891011Compiled Failed Invalid Time FailedType FailedMethod 302 0 0 1.27 0.....................................................######################## 术语分隔符 #########################Compiled 编译任务的执行次数#Failed 编译任务的失败次数 #Invalid 编译任务无效的次数#Time 编译任务花费的时间#FailedType 最后一次编译错误的类型#FailedMethod 最后一次编译错误的类名和方法 -gc：12345678910111213141516171819202122232425#每隔2秒监控一次，共20次 jstat -gc 2058 2000 20############################## S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 8704.0 8704.0 805.5 0.0 69952.0 64174.5 174784.0 2644.5 16384.0 10426.7 2 0.034 0 0.000 0.0348704.0 8704.0 805.5 0.0 69952.0 64174.5 174784.0 2644.5 16384.0 10426.7 2 0.034 0 0.000 0.0348704.0 8704.0 805.5 0.0 69952.0 64174.5 174784.0 2644.5 16384.0 10426.7 2 0.034 0 0.000 0.034.............................................######################## 术语分隔符 #########################S0C 生还者区0 容量(KB)#S1C 生还者区1 容量(KB)#S0U 生还者区0 使用量(KB)#S1U 生还者区1 使用量(KB)#EC 伊甸园区容量(KB)#EU 伊甸园区使用量(KB) #OC 老年区容量(KB)#OU 老年区使用量(KB)#PC 永久区容量(KB) #PU 永久区使用量(KB)#YGC 新生代GC次数#YGCT 新生代GC时间#FGC full GC 事件的次数#FGCT full GC的时间 #GCT 总GC时间 -gccapacity12345678910111213141516171819NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC PGCMN PGCMX PGC PC YGC FGC131072.0 131072.0 131072.0 13056.0 13056.0 104960.0 393216.0 393216.0 393216.0 393216.0 65536.0 65536.0 65536.0 65536.0 1 0..........................................................................................................######################## 术语分隔符 #########################NGCMN 最小新生代容量(KB)#NGCMX 最大新生代容量(KB)#NGC 当前新生代容量(KB)#S0C 当前生存者0区容量(KB)#S1C 当前生存者1区容量(KB)#OGCMN 老年代最小容量(KB)#OGCMX 老年代最大容量(KB)#OGC 当前老年代容量(KB). #OC 当前老年代？Current old space capacity (KB). #PGCMN 永久区最小容量(KB)#PGCMX 永久区最大容量(KB)#PGC 当前永久区容量(KB). #PC 当前永久区？Current Permanent space capacity (KB). #YGC young GC事件的次数 #FGC Full GC次数 -gccause1234567891011121314151617S0 S1 E O P YGC YGCT FGC FGCT GCT LGCC GCC 0.00 99.84 12.76 0.92 46.23 1 0.016 0 0.000 0.016 unknown GCCause No GC................................................######################## 术语分隔符 #########################S0 年轻代中第一个survivor（幸存区）已使用的占当前容量百分比#S1 年轻代中第二个survivor（幸存区）已使用的占当前容量百分比#E 年轻代中Eden（伊甸园）已使用的占当前容量百分比#O old代已使用的占当前容量百分比#P perm代已使用的占当前容量百分比#YGC 从应用程序启动到采样时年轻代中gc次数#FGC 从应用程序启动到采样时old代(全gc)gc次数#FGCT 从应用程序启动到采样时old代(全gc)gc所用时间(s)#GCT 从应用程序启动到采样时gc用的总时间(s)#LGCC 最后一次GC的原因#GCC 当前GC的原因#M：元数据区使用比例#CCS：压缩使用比例 例如 -gcutil ：123456789#每隔1秒监控一次，共10次jstat -gcutil 2058 1000 10################################[root@lq225 conf]# jstat -gcutil 2058 1000 10 S0 S1 E O P YGC YGCT FGC FGCT GCT 9.25 0.00 96.73 1.51 63.64 2 0.034 0 0.000 0.034 9.25 0.00 96.73 1.51 63.64 2 0.034 0 0.000 0.034 9.25 0.00 96.73 1.51 63.64 2 0.034 0 0.000 0.034 9.25 0.00 96.73 1.51 63.64 2 0.034 0 0.000 0.034 jmap12345678910#参数 -dump:[live,]format=b,file=&lt;filename&gt; 使用hprof二进制形式,输出jvm的heap内容到文件=. live子选项是可选的，假如指定live选项,那么只输出活的对象到文件. -finalizerinfo 打印正等候回收的对象的信息. -heap 打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. -permstat 打印classload和jvm heap长久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来. -F 强迫.在pid没有相应的时候使用-dump或者-histo参数. 在这个模式下,live子参数无效. -h | -help 打印辅助信息 -J 传递参数给jmap启动的jvm. pid 需要被打印配相信息的java进程id. 例如 -histo ：123456789jmap -histo 2058############################ num #instances #bytes class name---------------------------------------------- 1: 206 3585312 [I 2: 19621 2791880 &lt;constMethodKlass&gt; 3: 19621 2520048 &lt;methodKlass&gt; 4: 21010 2251616 [C............................................................ 例如 -dump：12345678#生成的文件可以使用jhat工具进行分析，在OOM（内存溢出）时，分析大对象，非常有用jmap -dump:live,format=b,file=data.hprof 2058#通过使用如下参数启动JVM，也可以获取到dump文件： -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./java_pid&lt;pid&gt;.hprof#如果在虚拟机中导出的heap信息文件可以拿到WINDOWS上进行分析，可以查找诸如内存方面的问题，可以这么做：jhat data.hprof #执行成功后，访问http://localhost:7000即可查看内存信息。（首先把7000端口打开） jinfo1234567891011121314151617#查看java进程的配置信息jinfo 2058#####################Attaching to process ID 2058, please wait...Debugger attached successfully.Server compiler detected.JVM version is 24.0-b56Java System Properties:java.runtime.name = Java(TM) SE Runtime Environmentproject.name = Amoeba-MySQLjava.vm.version = 24.0-b56sun.boot.library.path = /usr/local/java/jdk1.7/jre/lib/amd64................................................# 查看2058的MaxPerm大小可以用 jinfo -flag MaxPermSize 2058############################-XX:MaxPermSize=100663296 jps12345#列出系统中所有的java进程 jps#######################2306 Bootstrap3370 Jps 2058 xxxxxxxxx 一些术语的中文解释1234567891011121314151617181920212223242526272829303132333435S0C：年轻代中第一个survivor（幸存区）的容量 (字节)S1C：年轻代中第二个survivor（幸存区）的容量 (字节)S0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节)S1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节)EC：年轻代中Eden（伊甸园）的容量 (字节)EU：年轻代中Eden（伊甸园）目前已使用空间 (字节)OC：Old代的容量 (字节)OU：Old代目前已使用空间 (字节)PC：Perm(持久代)的容量 (字节)PU：Perm(持久代)目前已使用空间 (字节)YGC：从应用程序启动到采样时年轻代中gc次数YGCT：从应用程序启动到采样时年轻代中gc所用时间(s)FGC：从应用程序启动到采样时old代(全gc)gc次数FGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s)GCT：从应用程序启动到采样时gc用的总时间(s)NGCMN：年轻代(young)中初始化(最小)的大小 (字节)NGCMX：年轻代(young)的最大容量 (字节)NGC：年轻代(young)中当前的容量 (字节)OGCMN：old代中初始化(最小)的大小 (字节) OGCMX：old代的最大容量 (字节)OGC：old代当前新生成的容量 (字节)PGCMN：perm代中初始化(最小)的大小 (字节) PGCMX：perm代的最大容量 (字节) PGC：perm代当前新生成的容量 (字节)S0：年轻代中第一个survivor（幸存区）已使用的占当前容量百分比S1：年轻代中第二个survivor（幸存区）已使用的占当前容量百分比E：年轻代中Eden（伊甸园）已使用的占当前容量百分比O：old代已使用的占当前容量百分比P：perm代已使用的占当前容量百分比S0CMX：年轻代中第一个survivor（幸存区）的最大容量 (字节)S1CMX：年轻代中第二个survivor（幸存区）的最大容量 (字节)ECMX：年轻代中Eden（伊甸园）的最大容量 (字节)DSS：当前需要survivor（幸存区）的容量 (字节)（Eden区已满）TT： 持有次数限制MTT ： 最大持有次数限制 使用visualvm监控tomcat 修改catalina.sh，添加下面一行： 1234CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote=true -Djava.rmi.server.hostname=192.168.55.255 -Dcom.sun.management.jmxremote.port=8086 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"#注意点：#1、用hostname -i 查看是否为127.0.01，如果是，则必须配置-Djava.rmi.server.hostname为本机IP。#2、检查防火墙（iptables）是否开启，以及是否开放jmxremote.port所指定的端口。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>JVM调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[商城项目0-springboot初阶]]></title>
    <url>%2F2018%2F11%2F11%2F%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE1-springboot%E5%88%9D%E9%98%B6%2F</url>
    <content type="text"><![CDATA[0.学习目标 了解SpringBoot的作用 掌握java配置的方式 了解SpringBoot自动配置原理 掌握SpringBoot的基本使用 了解Thymeleaf的基本使用 1. 了解SpringBoot在这一部分，我们主要了解以下3个问题： 什么是SpringBoot 为什么要学习SpringBoot SpringBoot的特点 1.1.什么是SpringBootSpringBoot是Spring项目中的一个子工程，与我们所熟知的Spring-framework 同属于spring的产品: 我们可以看到下面的一段介绍： Takes an opinionated view of building production-ready Spring applications. Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible. 翻译一下： 用一些固定的方式来构建生产级别的spring应用。Spring Boot 推崇约定大于配置的方式以便于你能够尽可能快速的启动并运行程序。 其实人们把Spring Boot 称为搭建程序的脚手架。其最主要作用就是帮我们快速的构建庞大的spring项目，并且尽可能的减少一切xml配置，做到开箱即用，迅速上手，让我们关注与业务而非配置。 1.2.为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因注意是两点： 复杂的配置， 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 一个是混乱的依赖管理。 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这难题实在太棘手。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ Spring Boot 简化了基于Spring的应用开发，只需要“run”就能创建一个独立的、生产级别的Spring应用。Spring Boot为Spring平台及第三方库提供开箱即用的设置（提供默认设置，存放默认配置的包就是启动器），这样我们就可以简单的开始。多数Spring Boot应用只需要很少的Spring配置。 我们可以使用SpringBoot创建java应用，并使用java –jar 启动它，就能得到一个生产级别的web工程。 1.3.SpringBoot的特点Spring Boot 主要目标是： 为所有 Spring 的开发者提供一个非常快速的、广泛接受的入门体验 开箱即用（启动器starter-其实就是SpringBoot提供的一个jar包），但通过自己设置参数（.properties），即可快速摆脱这种方式。 提供了一些大型项目中常见的非功能性特性，如内嵌服务器、安全、指标，健康检测、外部化配置等 绝对没有代码生成，也无需 XML 配置。 更多细节，大家可以到官网查看。 2.快速入门接下来，我们就来利用SpringBoot搭建一个web工程，体会一下SpringBoot的魅力所在！ 2.1.创建工程我们先新建一个空的工程： 工程名为demo； 新建一个model： 使用maven来构建： 然后填写项目坐标： 目录结构： 项目结构： 2.2.添加依赖看到这里很多同学会有疑惑，前面说传统开发的问题之一就是依赖管理混乱，怎么这里我们还需要管理依赖呢？难道SpringBoot不帮我们管理吗？ 别着急，现在我们的项目与SpringBoot还没有什么关联。SpringBoot提供了一个名为spring-boot-starter-parent的工程，里面已经对各种常用依赖（并非全部）的版本进行了管理，我们的项目需要以这个项目为父工程，这样我们就不用操心依赖的版本问题了，需要什么依赖，直接引入坐标即可！ 2.2.1.添加父工程坐标12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 2.2.2.添加web启动器为了让SpringBoot帮我们完成各种自动配置，我们必须引入SpringBoot提供的自动配置依赖，我们称为启动器。因为我们是web项目，这里我们引入web启动器： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意的是，我们并没有在这里指定版本信息。因为SpringBoot的父工程已经对版本进行了管理了。 这个时候，我们会发现项目中多出了大量的依赖： 这些都是SpringBoot根据spring-boot-starter-web这个依赖自动引入的，而且所有的版本都已经管理好，不会出现冲突。 2.2.3.管理jdk版本默认情况下，maven工程的jdk版本是1.5，而我们开发使用的是1.8，因此这里我们需要修改jdk版本，只需要简单的添加以下属性即可： 123&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt; 2.2.4.完整pom123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leyou.demo&lt;/groupId&gt; &lt;artifactId&gt;springboot-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.3.启动类Spring Boot项目通过main函数即可启动，我们需要创建一个启动类： 然后编写main函数： 123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.4.编写controller接下来，我们就可以像以前那样开发SpringMVC的项目了！ 我们编写一个controller： 代码： 12345678@RestControllerpublic class HelloController &#123; @GetMapping("hello") public String hello()&#123; return "hello, spring boot!"; &#125;&#125; 2.5.启动测试接下来，我们运行main函数，查看控制台： 并且可以看到监听的端口信息： 1）监听的端口是8080 2）SpringMVC的映射路径是：/ 3）/hello路径已经映射到了HelloController中的hello()方法 打开页面访问：http://localhost:8080/hello 测试成功了！ 3.Java配置在入门案例中，我们没有任何的配置，就可以实现一个SpringMVC的项目了，快速、高效！ 但是有同学会有疑问，如果没有任何的xml，那么我们如果要配置一个Bean该怎么办？比如我们要配置一个数据库连接池，以前会这么玩： 1234567&lt;!-- 配置连接池 --&gt;&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;" /&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;" /&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;" /&gt;&lt;/bean&gt; 现在该怎么做呢？ 3.1.回顾历史事实上，在Spring3.0开始，Spring官方就已经开始推荐使用java配置来代替传统的xml配置了，我们不妨来回顾一下Spring的历史： Spring1.0时代 在此时因为jdk1.5刚刚出来，注解开发并未盛行，因此一切Spring配置都是xml格式，想象一下所有的bean都用xml配置，细思极恐啊，心疼那个时候的程序员2秒 Spring2.0时代 Spring引入了注解开发，但是因为并不完善，因此并未完全替代xml，此时的程序员往往是把xml与注解进行结合，貌似我们之前都是这种方式。 Spring3.0及以后 3.0以后Spring的注解已经非常完善了，因此Spring推荐大家使用完全的java配置来代替以前的xml，不过似乎在国内并未推广盛行。然后当SpringBoot来临，人们才慢慢认识到java配置的优雅。 有句古话说的好：拥抱变化，拥抱未来。所以我们也应该顺应时代潮流，做时尚的弄潮儿，一起来学习下java配置的玩法。 3.2.尝试java配置java配置主要靠java类和一些注解，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @value：属性注入 @PropertySource：指定外部属性文件， 我们接下来用java配置来尝试实现连接池配置： 首先引入Druid连接池依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 创建一个jdbc.properties文件，编写jdbc属性： 1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/leyoujdbc.username=rootjdbc.password=123 然后编写代码： 1234567891011121314151617181920212223@Configuration@PropertySource("classpath:jdbc.properties")public class JdbcConfig &#123; @Value("$&#123;jdbc.url&#125;") String url; @Value("$&#123;jdbc.driverClassName&#125;") String driverClassName; @Value("$&#123;jdbc.username&#125;") String username; @Value("$&#123;jdbc.password&#125;") String password; @Bean public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125;&#125; 解读： @Configuration：声明我们JdbcConfig是一个配置类 @PropertySource：指定属性文件的路径是:classpath:jdbc.properties 通过@Value为属性注入值 通过@Bean将 dataSource()方法声明为一个注册Bean的方法，Spring会自动调用该方法，将方法的返回值加入Spring容器中。 然后我们就可以在任意位置通过@Autowired注入DataSource了！ 我们在HelloController中测试： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private DataSource dataSource; @GetMapping("hello") public String hello() &#123; return "hello, spring boot!" + dataSource; &#125;&#125; 然后Debug运行并查看： 属性注入成功了！ 3.3.SpringBoot的属性注入在上面的案例中，我们实验了java配置方式。不过属性注入使用的是@Value注解。这种方式虽然可行，但是不够强大，因为它只能注入基本类型值。 在SpringBoot中，提供了一种新的属性注入方式，支持各种java基本数据类型及复杂类型的注入。 1）我们新建一个类，用来进行属性注入： 123456789@ConfigurationProperties(prefix = "jdbc")public class JdbcProperties &#123; private String url; private String driverClassName; private String username; private String password; // ... 略 // getters 和 setters&#125; 在类上通过@ConfigurationProperties注解声明当前类为属性读取类 prefix=&quot;jdbc&quot;读取属性文件中，前缀为jdbc的值。 在类上定义各个属性，名称必须与属性文件中jdbc.后面部分一致 需要注意的是，这里我们并没有指定属性文件的地址，所以我们需要把jdbc.properties名称改为application.properties，这是SpringBoot默认读取的属性文件名： 2）在JdbcConfig中使用这个属性： 1234567891011121314@Configuration@EnableConfigurationProperties(JdbcProperties.class)public class JdbcConfig &#123; @Bean public DataSource dataSource(JdbcProperties jdbc) &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(jdbc.getUrl()); dataSource.setDriverClassName(jdbc.getDriverClassName()); dataSource.setUsername(jdbc.getUsername()); dataSource.setPassword(jdbc.getPassword()); return dataSource; &#125;&#125; 通过@EnableConfigurationProperties(JdbcProperties.class)来声明要使用JdbcProperties这个类的对象 然后你可以通过以下方式注入JdbcProperties： @Autowired注入 12@Autowiredprivate JdbcProperties prop; 构造函数注入 1234private JdbcProperties prop;public JdbcConfig(Jdbcproperties prop)&#123; this.prop = prop;&#125; 声明有@Bean的方法参数注入 1234@Beanpublic Datasource dataSource(JdbcProperties prop)&#123; // ...&#125; 本例中，我们采用第三种方式。 3）测试结果： 大家会觉得这种方式似乎更麻烦了，事实上这种方式有更强大的功能，也是SpringBoot推荐的注入方式。两者对比关系： 优势： Relaxed binding：松散绑定 不严格要求属性文件中的属性名与成员变量名一致。支持驼峰，中划线，下划线等等转换，甚至支持对象引导。比如：user.friend.name：代表的是user对象中的friend属性中的name属性，显然friend也是对象。@value注解就难以完成这样的注入方式。 meta-data support：元数据支持，帮助IDE生成属性提示（写开源框架会用到）。 ​ 3.4、更优雅的注入事实上，如果一段属性只有一个Bean需要使用，我们无需将其注入到一个类（JdbcProperties）中。而是直接在需要的地方声明即可： 1234567891011@Configurationpublic class JdbcConfig &#123; @Bean // 声明要注入的属性前缀，SpringBoot会自动把相关属性通过set方法注入到DataSource中 @ConfigurationProperties(prefix = "jdbc") public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); return dataSource; &#125;&#125; 我们直接把@ConfigurationProperties(prefix = &quot;jdbc&quot;)声明在需要使用的@Bean的方法上，然后SpringBoot就会自动调用这个Bean（此处是DataSource）的set方法，然后完成注入。使用的前提是：该类必须有对应属性的set方法！ 我们将jdbc的url改成：/heima，再次测试： 4.自动配置原理使用SpringBoot之后，一个整合了SpringMVC的WEB工程开发，变的无比简单，那些繁杂的配置都消失不见了，这是如何做到的？ 一切魔力的开始，都是从我们的main函数来的，所以我们再次来看下启动类： 我们发现特别的地方有两个： 注解：@SpringBootApplication run方法：SpringApplication.run() 我们分别来研究这两个部分。 4.1.了解@SpringBootApplication点击进入，查看源码： 这里重点的注解有3个： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 4.1.1.@SpringBootConfiguration我们继续点击查看源码： 通过这段我们可以看出，在这个注解上面，又有一个@Configuration注解。通过上面的注释阅读我们知道：这个注解的作用就是声明当前类是一个配置类，然后Spring会自动扫描到添加了@Configuration的类，并且读取其中的配置信息。而@SpringBootConfiguration是来声明当前类是SpringBoot应用的配置类，项目中只能有一个。所以一般我们无需自己添加。 4.1.2.@EnableAutoConfiguration关于这个注解，官网上有一段说明： The second class-level annotation is @EnableAutoConfiguration. This annotationtells Spring Boot to “guess” how you want to configure Spring, based on the jardependencies that you have added. Since spring-boot-starter-web added Tomcatand Spring MVC, the auto-configuration assumes that you are developing a webapplication and sets up Spring accordingly. 简单翻译以下： 第二级的注解@EnableAutoConfiguration，告诉SpringBoot基于你所添加的依赖，去“猜测”你想要如何配置Spring。比如我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖。此时自动配置就知道你是要开发一个web应用，所以就帮你完成了web及SpringMVC的默认配置了！ 总结，SpringBoot内部对大量的第三方库或Spring内部库进行了默认配置，这些配置是否生效，取决于我们是否引入了对应库所需的依赖，如果有那么默认配置就会生效。 所以，我们使用SpringBoot构建一个项目，只需要引入所需框架的依赖，配置就可以交给SpringBoot处理了。除非你不希望使用SpringBoot的默认配置，它也提供了自定义配置的入口。 4.1.3.@ComponentScan我们跟进源码： 并没有看到什么特殊的地方。我们查看注释： 大概的意思： 配置组件扫描的指令。提供了类似与&lt;context:component-scan&gt;标签的作用 通过basePackageClasses或者basePackages属性来指定要扫描的包。如果没有指定这些属性，那么将从声明这个注解的类所在的包开始，扫描包及子包 而我们的@SpringBootApplication注解声明的类就是main函数所在的启动类，因此扫描的包是该类所在包及其子包。因此，一般启动类会放在一个比较前的包目录中。 4.2.默认配置原理4.2.1默认配置类通过刚才的学习，我们知道@EnableAutoConfiguration会开启SpringBoot的自动配置，并且根据你引入的依赖来生效对应的默认配置。那么问题来了： 这些默认配置是在哪里定义的呢？ 为何依赖引入就会触发配置呢？ 其实在我们的项目中，已经引入了一个依赖：spring-boot-autoconfigure，其中定义了大量自动配置类： 还有： 非常多，几乎涵盖了现在主流的开源框架，例如： redis jms amqp jdbc jackson mongodb jpa solr elasticsearch … 等等 我们来看一个我们熟悉的，例如SpringMVC，查看mvc 的自动配置类： 打开WebMvcAutoConfiguration： 我们看到这个类上的4个注解： @Configuration：声明这个类是一个配置类 @ConditionalOnWebApplication(type = Type.SERVLET) ConditionalOn，翻译就是在某个条件下，此处就是满足项目的类是是Type.SERVLET类型，也就是一个普通web工程，显然我们就是 @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) 这里的条件是OnClass，也就是满足以下类存在：Servlet、DispatcherServlet、WebMvcConfigurer，其中Servlet只要引入了tomcat依赖自然会有，后两个需要引入SpringMVC才会有。这里就是判断你是否引入了相关依赖，引入依赖后该条件成立，当前类的配置才会生效！ @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 这个条件与上面不同，OnMissingBean，是说环境中没有指定的Bean这个才生效。其实这就是自定义配置的入口，也就是说，如果我们自己配置了一个WebMVCConfigurationSupport的类，那么这个默认配置就会失效！ 接着，我们查看该类中定义了什么： 视图解析器： 处理器适配器（HandlerAdapter）： 还有很多，这里就不一一截图了。 4.2.2.默认配置属性另外，这些默认配置的属性来自哪里呢？ 我们看到，这里通过@EnableAutoConfiguration注解引入了两个属性：WebMvcProperties和ResourceProperties。这不正是SpringBoot的属性注入玩法嘛。 我们查看这两个属性类： 找到了内部资源视图解析器的prefix和suffix属性。 ResourceProperties中主要定义了静态资源（.js,.html,.css等)的路径： 如果我们要覆盖这些默认属性，只需要在application.properties中定义与其前缀prefix和字段名一致的属性即可。 4.3.总结SpringBoot为我们提供了默认配置，而默认配置生效的条件一般有两个： 你引入了相关依赖 你自己没有配置 1）启动器 所以，我们如果不想配置，只需要引入依赖即可，而依赖版本我们也不用操心，因为只要引入了SpringBoot提供的stater（启动器），就会自动管理依赖及版本了。 因此，玩SpringBoot的第一件事情，就是找启动器，SpringBoot提供了大量的默认启动器，参考课前资料中提供的《SpringBoot启动器.txt》 2）全局配置 另外，SpringBoot的默认配置，都会读取默认属性，而这些属性可以通过自定义application.properties文件来进行覆盖。这样虽然使用的还是默认配置，但是配置中的值改成了我们自定义的。 因此，玩SpringBoot的第二件事情，就是通过application.properties来覆盖默认属性值，形成自定义配置。我们需要知道SpringBoot的默认属性key，非常多，参考课前资料提供的：《SpringBoot全局属性.md》 5.SpringBoot实践接下来，我们来看看如何用SpringBoot来玩转以前的SSM,我们沿用之前讲解SSM用到的数据库tb_user和实体类User 5.1.整合SpringMVC虽然默认配置已经可以使用SpringMVC了，不过我们有时候需要进行自定义配置。 5.1.1.修改端口查看SpringBoot的全局属性可知，端口通过以下方式配置： 12# 映射端口server.port=80 重启服务后测试： 5.1.2.访问静态资源现在，我们的项目是一个jar工程，那么就没有webapp，我们的静态资源该放哪里呢？ 回顾我们上面看的源码，有一个叫做ResourceProperties的类，里面就定义了静态资源的默认查找路径： 默认的静态资源路径为： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public 只要静态资源放在这些目录中任何一个，SpringMVC都会帮我们处理。 我们习惯会把静态资源放在classpath:/static/目录下。我们创建目录，并且添加一些静态资源： 重启项目后测试： 5.1.3.添加拦截器拦截器也是我们经常需要使用的，在SpringBoot中该如何配置呢？ 拦截器不是一个普通属性，而是一个类，所以就要用到java配置方式了。在SpringBoot官方文档中有这么一段说明： If you want to keep Spring Boot MVC features and you want to add additional MVC configuration (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, you can declare a WebMvcRegistrationsAdapter instance to provide such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 翻译： 如果你想要保持Spring Boot 的一些默认MVC特征，同时又想自定义一些MVC配置（包括：拦截器，格式化器, 视图控制器、消息转换器 等等），你应该让一个类实现WebMvcConfigurer，并且添加@Configuration注解，但是千万不要加@EnableWebMvc注解。如果你想要自定义HandlerMapping、HandlerAdapter、ExceptionResolver等组件，你可以创建一个WebMvcRegistrationsAdapter实例 来提供以上组件。 如果你想要完全自定义SpringMVC，不保留SpringBoot提供的一切特征，你可以自己定义类并且添加@Configuration注解和@EnableWebMvc注解 总结：通过实现WebMvcConfigurer并添加@Configuration注解来实现自定义部分SpringMvc配置。 首先我们定义一个拦截器： 12345678910111213141516171819public class LoginInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(LoginInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; logger.debug("preHandle method is now running!"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) &#123; logger.debug("postHandle method is now running!"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; logger.debug("afterCompletion method is now running!"); &#125;&#125; 然后，我们定义配置类，注册拦截器： 123456789101112131415161718192021@Configurationpublic class MvcConfig implements WebMvcConfigurer&#123; /** * 通过@Bean注解，将我们定义的拦截器注册到Spring容器 * @return */ @Bean public LoginInterceptor loginInterceptor()&#123; return new LoginInterceptor(); &#125; /** * 重写接口中的addInterceptors方法，添加自定义拦截器 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 通过registry来注册拦截器，通过addPathPatterns来添加拦截路径 registry.addInterceptor(this.loginInterceptor()).addPathPatterns("/**"); &#125;&#125; 结构如下： 接下来运行并查看日志： 你会发现日志中什么都没有，因为我们记录的log级别是debug，默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.*=debug来配置日志级别，*填写包名 12# 设置com.leyou包的日志级别为debuglogging.level.com.leyou=debug 再次运行查看： 1232018-05-05 17:50:01.811 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : preHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : postHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : afterCompletion method is now running! 如果不想每次都写private final Logger logger = LoggerFactory.getLogger(XXX.class); 可以用注解@Slf4j： 1234&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt;&lt;/dependency&gt; 注意：如果注解@Slf4j注入后找不到变量log，那就给IDE安装lombok插件，、 下面以idea为例 1、File → settings → Plugins, 然后点击“Browse repositories” 如图 2、输入 lombok 搜索插件， 点install安装，安装完重启idea 3.之后就可以直接用注解的方式调日志啦 5.2.整合jdbc和事务spring中的jdbc连接和事务是配置中的重要一环，在SpringBoot中该如何处理呢？ 答案是不需要处理，我们只要找到SpringBoot提供的启动器即可： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 当然，不要忘了数据库驱动，SpringBoot并不知道我们用的什么数据库，这里我们选择MySQL： 1234&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 至于事务，SpringBoot中通过注解来控制。就是我们熟知的@Transactional 当然，在后文中整合了通用mapper的依赖后，他会自动帮我们引好jdbc和mysql的依赖 5.3.整合连接池其实，在刚才引入jdbc启动器的时候，SpringBoot已经自动帮我们引入了一个连接池： HikariCP应该是目前速度最快的连接池了，我们看看它与c3p0的对比： 因此，我们只需要指定连接池参数即可： 12345678910# 连接四大参数spring.datasource.url=jdbc:mysql://localhost:3306/heimaspring.datasource.username=rootspring.datasource.password=123# 可省略，SpringBoot自动推断spring.datasource.driverClassName=com.mysql.jdbc.Driverspring.datasource.hikari.idle-timeout=60000spring.datasource.hikari.maximum-pool-size=30spring.datasource.hikari.minimum-idle=10 当然，如果你更喜欢Druid连接池，也可以使用Druid官方提供的启动器： 123456&lt;!-- Druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 而连接信息的配置与上面是类似的，只不过在连接池特有属性上，方式略有不同： 12345678910#初始化连接数spring.datasource.druid.initial-size=1#最小空闲连接spring.datasource.druid.min-idle=1#最大活动连接spring.datasource.druid.max-active=20#获取连接时测试是否可用spring.datasource.druid.test-on-borrow=true#监控页面启动spring.datasource.druid.stat-view-servlet.allow=true 5.4.整合mybatis5.4.1.mybatisSpringBoot官方并没有提供Mybatis的启动器，不过Mybatis官网自己实现了： 123456&lt;!--mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 配置，基本没有需要配置的： 1234# mybatis 别名扫描mybatis.type-aliases-package=com.heima.pojo# mapper.xml文件位置,如果没有映射文件，请注释掉，单表的话可以使用通用mapper就不需配置此条mybatis.mapper-locations=classpath:mappers/*.xml 需要注意，这里没有配置mapper接口扫描包，因此我们需要给每一个Mapper接口添加@Mapper注解，才能被识别。 123@Mapperpublic interface UserMapper &#123;&#125; 可以在启动类上直接集成 5.4.2.通用mapper通用Mapper的作者也为自己的插件编写了启动器，我们直接引入即可： 123456&lt;!-- 通用mapper --&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 不需要做任何配置就可以使用了。 123@Mapperpublic interface UserMapper extends tk.mybatis.mapper.common.Mapper&lt;User&gt;&#123;&#125; ctrl+F12就可以查看当前类继承了哪些方法 pom文件中引入了通用mapper后，mybatis和jdbc就无需单独再引依赖了 通用mapper中的User： 123456789101112131415161718192021222324252627@Data@Table(name = "tb_user")//默认会将类名当表名,配置好可以告诉他应该扫哪张表public class User &#123; @Id//告诉他Id是主键 @KeySql(useGeneratedKeys = true)//告诉他主键自增 private Long id; private String userName; private String password; private String name; private Integer age; private Integer sex; private Date birthday; private Date created; private Date updated; @Transient//这个注解标注后不会持久化到数据库，是瞬时变量 private String note;&#125; 引入test启动器 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt; 直接在UserMapper接口上Alt+Enter创建测试类 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; @Test public void testQuery() &#123; User user = userMapper.selectByPrimaryKey(1L); System.out.println("user=" + user); &#125;&#125; 测试结果如下： 123452018-11-11 16:09:35.553 INFO 9152 --- [ main] com.alibaba.druid.pool.DruidDataSource : &#123;dataSource-1&#125; inited2018-11-11 16:09:36.036 DEBUG 9152 --- [ main] c.l.m.UserMapper.selectByPrimaryKey : ==&gt; Preparing: SELECT id,username,password,name,age,sex,birthday,created,updated FROM tb_user WHERE id = ? 2018-11-11 16:09:36.084 DEBUG 9152 --- [ main] c.l.m.UserMapper.selectByPrimaryKey : ==&gt; Parameters: 1(Long)2018-11-11 16:09:36.109 DEBUG 9152 --- [ main] c.l.m.UserMapper.selectByPrimaryKey : &lt;== Total: 1user=User(id=1, username=1, password=1, name=1, age=1, sex=1, birthday=Tue Nov 13 16:06:34 CST 2018, created=Tue Nov 06 16:06:39 CST 2018, updated=Sat Dec 08 16:06:43 CST 2018, note=null) 在业务里操作表数据，增删数据时用到了事务@Transactional： 123456789101112131415@Servicepublic class UserService &#123; @Autowired private UserMapper userMapper; public User queryById(Long id)&#123; return this.userMapper.selectByPrimaryKey(id); &#125; @Transactional public void deleteById(Long id)&#123; this.userMapper.deleteByPrimaryKey(id); &#125;&#125; 5.5.启动测试将controller进行简单改造： 12345678910111213141516171819@RestController@RequestMapping("user")public class HelloController &#123; @Autowired private DataSource dataSource; @Autowired private UserService userService; @GetMapping("hello") public String hello() &#123; return "hello,spring boot!" + dataSource; &#125; @GetMapping("&#123;id&#125;") public User find(@PathVariable("id") Long id) &#123; return userService.queryById(id); &#125;&#125; 6.Thymeleaf快速入门SpringBoot并不推荐使用jsp，但是支持一些模板引擎技术： 以前大家用的比较多的是Freemarker，但是我们今天的主角是Thymeleaf！ 6.1.为什么是Thymeleaf？简单说， Thymeleaf 是一个跟 Velocity、FreeMarker 类似的模板引擎，它可以完全替代 JSP 。相较与其他的模板引擎，它有如下三个极吸引人的特点： 动静结合：Thymeleaf 在有网络和无网络的环境下皆可运行，即它可以让美工在浏览器查看页面的静态效果，也可以让程序员在服务器查看带数据的动态页面效果。这是由于它支持 html 原型，然后在 html 标签里增加额外的属性来达到模板+数据的展示方式。浏览器解释 html 时会忽略未定义的标签属性，所以 thymeleaf 的模板可以静态地运行；当有数据返回到页面时，Thymeleaf 标签会动态地替换掉静态内容，使页面动态显示。 开箱即用：它提供标准和spring标准两种方言，可以直接套用模板实现JSTL、 OGNL表达式效果，避免每天套模板、该jstl、改标签的困扰。同时开发人员也可以扩展和创建自定义的方言。 多方言支持：Thymeleaf 提供spring标准方言和一个与 SpringMVC 完美集成的可选模块，可以快速的实现表单绑定、属性编辑器、国际化等功能。 与SpringBoot完美整合，SpringBoot提供了Thymeleaf的默认配置，并且为Thymeleaf设置了视图解析器，我们可以像以前操作jsp一样来操作Thymeleaf。代码几乎没有任何区别，就是在模板语法上有区别。 接下来，我们就通过入门案例来体会Thymeleaf的魅力： 6.2.编写接口编写一个controller，返回一些用户数据，放入模型中，等会在页面渲染 123456789@GetMapping("/all")public String all(ModelMap model) &#123; // 查询用户 List&lt;User&gt; users = this.userService.queryAll(); // 放入模型 model.addAttribute("users", users); // 返回模板名称（就是classpath:/templates/目录下的html文件名） return "users";&#125; 6.3.引入启动器直接引入启动器： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; SpringBoot会自动为Thymeleaf注册一个视图解析器： 与解析JSP的InternalViewResolver类似，Thymeleaf也会根据前缀和后缀来确定模板文件的位置： 默认前缀：classpath:/templates/ 默认后缀：.html 所以如果我们返回视图：users，会指向到 classpath:/templates/users.html 一般我们无需进行修改，默认即可。 6.4.静态页面根据上面的文档介绍，模板默认放在classpath下的templates文件夹，我们新建一个html文件放入其中： 编写html模板，渲染模型中的数据： 注意，把html 的名称空间，改成：xmlns:th=&quot;http://www.thymeleaf.org&quot; 会有语法提示 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;首页&lt;/title&gt; &lt;style type="text/css"&gt; table &#123;border-collapse: collapse; font-size: 14px; width: 80%; margin: auto&#125; table, th, td &#123;border: 1px solid darkslategray;padding: 10px&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div style="text-align: center"&gt; &lt;span style="color: darkslategray; font-size: 30px"&gt;欢迎光临！&lt;/span&gt; &lt;hr/&gt; &lt;table class="list"&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;用户名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;性别&lt;/th&gt; &lt;th&gt;生日&lt;/th&gt; &lt;th&gt;备注&lt;/th&gt; &lt;/tr&gt; &lt;tr th:each="user : $&#123;users&#125;"&gt; &lt;td th:text="$&#123;user.id&#125;"&gt;1&lt;/td&gt; &lt;td th:text="$&#123;user.name&#125;"&gt;张三&lt;/td&gt; &lt;td th:text="$&#123;user.userName&#125;"&gt;zhangsan&lt;/td&gt; &lt;td th:text="$&#123;user.age&#125;"&gt;20&lt;/td&gt; &lt;td th:text="$&#123;user.sex&#125; == 1 ? '男': '女'"&gt;男&lt;/td&gt; &lt;td th:text="$&#123;#dates.format(user.birthday, 'yyyy-MM-dd')&#125;"&gt;1980-02-30&lt;/td&gt; &lt;td th:text="$&#123;user.note&#125;"&gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 我们看到这里使用了以下语法： ${} ：这个类似与el表达式，但其实是ognl的语法，比el表达式更加强大 th-指令：th-是利用了Html5中的自定义属性来实现的。如果不支持H5，可以用data-th-来代替 th:each：类似于c:foreach 遍历集合，但是语法更加简洁 th:text：声明标签中的文本 例如&lt;td th-text=&#39;${user.id}&#39;&gt;1&lt;/td&gt;，如果user.id有值，会覆盖默认的1 如果没有值，则会显示td中默认的1。这正是thymeleaf能够动静结合的原因，模板解析失败不影响页面的显示效果，因为会显示默认值！ 6.5.测试接下来，我们打开页面测试一下： 123456789注意：一开始在controller类中用的是@RestController这个注解，这样return的就只是字符串，并不是我们想要的/templates/**.html的格式，此处将其改为@Controller即可。@RestController注解相当于@ResponseBody ＋ @Controller合在一起的作用。1)如果只是使用@RestController注解Controller，则Controller中的方法无法返回jsp页面，配置的视图解析器InternalResourceViewResolver不起作用，返回的内容就是Return 里的内容。例如：本来应该到success.jsp页面的，则其显示success.2)如果需要返回到指定页面，则需要用 @Controller配合视图解析器InternalResourceViewResolver才行。3)如果需要返回JSON，XML或自定义mediaType内容到页面，则需要在对应的方法上加上@ResponseBody注解。 6.6.模板缓存Thymeleaf会在第一次对模板解析之后进行缓存，极大的提高了并发处理能力。但是这给我们开发带来了不便，修改页面后并不会立刻看到效果，我们开发阶段可以关掉缓存使用： 12# 开发阶段关闭thymeleaf的模板缓存spring.thymeleaf.cache=false 注意： 1在Idea中，我们需要在修改页面后按快捷键：`Ctrl + Shift + F9` 对项目进行rebuild才可以。 我们可以修改页面，测试一下。]]></content>
      <categories>
        <category>商城项目</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>Thymeleaf</tag>
        <tag>通用Mapper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法——排序]]></title>
    <url>%2F2018%2F11%2F05%2F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1.插入排序1234567891011121314public static void sort(int[] n)&#123; int k; for (int i = 1; i &lt;n.length ; i++) &#123;//遍历数组，从1下标开始 //用temp暂存要排序的值 int temp=n[i]; //待处理的数据，在已排好的数组中从后往前进行比较遍历 //条件是：待处理数据是否比有序的数据中某值小，如果小就要数组进行移动 for ( k = i; k &gt;0&amp;&amp;temp&lt;n[k-1] ; k--) &#123; //数组移位，位数据的插入做准备 n[k]=n[k-1]; &#125; n[k]=temp; &#125; &#125; 2.快速排序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class QuickSort &#123; //最容易理解的快速排序 //我们以int排序为例，方便起见，就不像以前一样实现对象的compareTo方法了 public static void sort(List&lt;Integer&gt; list)&#123; //递归结束条件 if(list.size() &gt; 1)&#123;//上面说的，当数据量为1或者0的时候结束递归 //建立三个集合，分别表示小于枢纽元，大于枢纽元和等于枢纽元 List&lt;Integer&gt; smallerList = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; largerList = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; sameList = new ArrayList&lt;Integer&gt;(); //选取一个随机值作为枢纽元，在我们学习过程中，我们通常把第一个数作为枢纽元 Integer pivot = list.get(0); //遍历list，把比pivot小的放smallerList中，比pivot大的放largerList中,相等的放sameList中 for (Integer integer : list) &#123; if(integer &lt; pivot)&#123; smallerList.add(integer); &#125; else if(integer &gt; pivot)&#123; largerList.add(integer); &#125;else&#123; sameList.add(integer); &#125; &#125; //递归实现分组后的子数据进行上面同样的操作 sort(smallerList); sort(largerList); //对排序好的数据进行整合 list.clear();//清楚原本的数据，用于存放有序的数据 list.addAll(smallerList); list.addAll(sameList); list.addAll(largerList); &#125; &#125; public static void main(String[] args) &#123; Integer[] target = &#123;4,3,6,7,1,9,5,2,3,3&#125;; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; target.length; i++) &#123; list.add(target[i]); &#125; sort(list); //查看排序结果 for (Integer integer : list) &#123; System.out.print(integer + " "); &#125; &#125;&#125;//输出结果：//1 2 3 3 3 4 5 6 7 9 // 为什么我要用一个List来存储一样大小的呢？ 这里就暴露出了快速排序算法中对相同数据的处理方式。在上述的实现中为什么不直接把相等的放入smallerList或者放入largerList呢？举个例子：假如说你选取的枢纽元是最小值，那么是不是可能发生每次递归的数据都是一样的？因为所有的数据都比你的枢纽元大，而且这个时候恰好你把相等的数据都放入了比枢纽元大的部分，那么就会造成栈溢出了。所以在这个过程中，我们需要对相等的数据单独存储起来。 如何选取枢纽元1、以第一个值为枢纽元在大学期间，我们学习到快速排序，告知我们的一般都是以第一个值为枢纽元。对于这种默认的选取方式，我们对他进行剖析：①如果待排序的数据是随机的，那么如此选择枢纽元是可以接受的，因为在概率上来说，随机的情况下，在第一次快速排序之后，会分为两个差不多相等的新的数据。②如果待排序的数据是有序的，那么这种情况下，就不能以第一个值为枢纽元了，因为它会产生一种恶心分割，直接导致所有的元素都被划分到左边子数据或者右边子数据。所以这种办法是不可取的，也尽量不去实现它。也有人说可以选取第一个和第二个数据做比较，比较大的作为枢纽元。这种方式只是简单的规避了划分为空的情况，这种恶心的划分还是存在的。 2、随机选择一个枢纽元在待排序的数据中随机选取一个数据为枢纽元会显得安全很多。它的随机可以保证在分割的过程中可以合理、平均的进行划分。但是我们要考虑随机数产生的开销，每趟分割之前还需要随机出一个随机数，那么开销会变得非常巨大。所以这种方式虽然比较安全，但是性能仍旧是不可取的。 3、三数中值分割法三数中值分割法是目前比较高效的一种选取枢纽元的方式。按照选取枢纽元的要求：要尽可能合理、平均的划分为两部分。那么最好的是选取一个中值，那么就可以精准的分割两部分了。但是我们不可能划分这个开销去寻找中值，我们做的只是:我们从待排序的数据中选取3个位置的值，分别是第一位置、最后位置、中间位置。然后我们用这3个数据中排序中间的数据作为枢纽元。 我们在选取好枢纽元之后仍旧需要遍历3个之中剩余的两个值，因为这里已经比较了一轮，我们只需要在枢纽元选取的时候就把剩余两个进行排序了，比枢纽元小的放在最前面，比枢纽元大的放在最后面，且在遍历的时候跳过这两个值。所以说三数中值分割法并没有白白花费这个效率开销。 我们要对数据：3，2，5，7，1，8，9进行快速排序。我们随机选取一个值为枢纽元：5，那么我们就把5与9进行位置交换，把枢纽元独立到数据的边缘，避免它参与数据交换，所以初始情况就是这样：3，2，9，7，1，8，5我们定义两个指针，这两个指针我们称为头指针和尾指针，分别指向第一个元素和倒数第二个元素（倒数第一个元素是枢纽元）。接着就需要开始移动指针：在头指针指向的位置小于尾指针指向的位置时：①我们将头指针向右移动，遇到比枢纽元小的元素直接移动到下一个数据，直到遇到一个数据大于枢纽元，则头指针停止运动。②相同的，移动尾指针向前移动，遇到比枢纽大的元素直接移动到下一个数据，直到遇到一个数据小于枢纽元。③等两个指针都停止下来的时候，就需要把两个指针所指向的数据进行交换。并继续重复①②③步骤。④直到尾指针指向的位置小于头指针指向的位置，通俗来说就是两个指针交错了就结束遍历。 下面是对于上面数据快速排序的一趟图解: 在此基础上，我们再来说说为什么三数中值分割法并没有浪费额外的开销：用三数中值分割法获取到枢纽元，那么其实比这个枢纽元小的数据可以放在最左边，比这个枢纽元大的数据放在最右边。那么这样一来，头指针就可以从第二个开始，尾指针就可以从倒数第二个开始，这样一来，一定程度上效率是有回升的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class QuickSort &#123; //暴露给外部的接口，对一个数组进行排序 public static void quicksort(Integer [] target)&#123; quicksoort(target, 0, target.length - 1); &#125; //具体实现 //用left与right的方式，尽可能的实现数组复用 private static void quicksoort(Integer[] target, int left, int right)&#123; if(left + 2 &lt; right)&#123;//递归结束条件,之所以+2是因为三数中值分割法最起码需要两个数据 //寻找枢纽元 int pivot = findPivot(target, left, right, false); //定义头指针与尾指针 int i = left + 1, j = right - 2; //因为三数中值分割法导致最前数据和最后数据不用判断 for( ; ; )&#123; //两个指针开始运动，直到两者都停止 while(target[i] &lt;= pivot)&#123;i++;&#125;//如果头指针遍历到小于枢纽元的数据直接跳过 while(target[j] &gt;= pivot)&#123;j--;&#125;//如果尾指针遍历到大于枢纽元的数据直接跳过 //判断两个指针是否交错 if(i &lt; j)&#123; //没有交错，且指针停止，那么进行数据交换 swap(target, i, j); &#125;else&#123; break;//指针交错，那么结束循环 &#125; &#125; //也就是上面描述的指针交错之后，需要把枢纽元交换到头指针的位置 swap(target, i, right - 1); //继续递归子数组 quicksoort(target, left, i - 1); quicksoort(target, i + 1, right); &#125;else&#123; //当数据少于2个的时候，直接用三数中值分割法进行排序 findPivot(target, left, right, true); &#125; &#125; //三数中值分割法 //这个判断用于说明是否最后的操作，最后的操作不需要把枢纽值放到最后 private static Integer findPivot(Integer[] target, int left, int right, boolean end)&#123; int mid = (left + right) / 2;//获取中间值的位置 //比较开始数据与中间数据 if(target[left] &gt; target[mid])&#123; //如果开始数据比中间数据大，那么位置进行交换 swap(target, left, mid); &#125; if(target[left] &gt; target[right])&#123; //如果开始的数据比最后数据大，那么交换位置 swap(target, left, right); &#125; if(target[mid] &gt; target[right])&#123; //如果中间的数据比最后的数据大，那么交换位置 swap(target, mid, right); &#125; if(!end)&#123; //按照前面说的，把枢纽元放到最后面 swap(target, mid, right - 1); //返回枢纽元 return target[right - 1]; &#125; return null; &#125; //交换数组中两个下标的数据 private static final void swap(Integer[] target, int one, int anthor)&#123; int temp = target[one]; target[one] = target[anthor]; target[anthor] = temp; &#125; public static void main(String[] args) &#123; Integer[] target = &#123;4,3,6,7,1,9,5,2,3,3&#125;; quicksort(target); for (Integer integer : target) &#123; System.out.print(integer + " "); &#125; &#125;&#125;//输出结果：//1 2 3 3 3 4 5 6 7 9 // 各种排序算法性能分析：]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>插入排序</tag>
        <tag>快速排序</tag>
        <tag>排序算法性能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——ConcurrentHashMap]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[技术点：1.悲观锁和乐观锁悲观锁是指如果一个进程占用了一个锁，而导致其他需要这个锁的线程进入等待，一直到该锁被释放，换句话说就是这个锁被独占，比如典型的synchronized； 乐观锁是指操作并不加锁，而是抱着尝试的心态去执行某项操作，如果操作失败或者操作冲突，那么就进入重试，一直到执行成功为止。 2.原子性，指令有序性和线程可见性这三个性质是多线程编程中核心的问题。 原子性和事务的原子性一样，对于一个或多个操作，要么都执行，要么都不执行。指令有序性是指，在我们编写的代码中，上下两个互不关联的语句不会被指令重排序。 指令重排序是指处理器为了性能优化，在无关联的代码的执行是可能会和代码顺序不一致。比如说int i=1;int j=2;那么这两条语句的执行顺序可能会先执行int j=2； 线程可见性是指一个线程修改了某个变量，其他线程马上能知道。 3.无锁算法使用低层原子化的机器指令，保证并发情况下数据的完整性，典型的如CAS算法。 4.内存屏障在《深入理解JVM》中解释：它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障之后；即在执行到内存屏障这句指令时，它前面的操作已经全部完成；它会强制对缓存的修改操作立即写入主存；如果是写操作，它会导致其他CPU中对应的缓存行无效。在使用volatile修饰的变量会产生内存屏障。 Java内存模型 1.解释：每个线程会从主内存中读取操作，所有的变量都存储在主内存中，每个线程都需要从主内存中获得变量的值。 然后如图可见，每个线程获得数据之后会放入自己的工作内存，这就是java内存模型的规定之二，保证每个线程操作的都是从主内存拷贝的副本，也就是说线程不能直接操作主内存中的变量，需要把主内存的变量值读取之后放入自己的工作内存中的变量副本中，然后操作这个副本。 最后线程和线程之间无法直接访问对方工作内存中的变量。最后需要解释一下这个访问规则局限于对象实例字段，静态字段等，局部变量不包括在内，因为局部变量不存在竞争问题。 2.基本执行步骤：a.lock(锁定)：在某一个线程在读取主内存的时候需要把变量锁定 b.unlock（解锁）：在某一个线程读取完变量值之后会释放锁定，别的线程就可以进入操作 c.read（读取）：从主内存中读取变量的值并放入工作内存中 d.load（加载）：从read操作中得到的值放入工作内存变量副本中 e.use（使用）：把工作内存中的一个变量值传递给执行引擎 f.assign（赋值）：它把一个从执行引擎接收到的值赋值给工作内存的变量 g.store（存储）：把工作内存中的一个变量的值传送到主内存中 h.write（写入）：把store操作从工作内存中的一个变量的值传送到主内存的变量中 volatile关键字在concurrentHashMap中，有很多成员变量都是用volatile修饰的，被volatile修饰的变量有如下特性： 1.使得变量更新变得具有可见性，只有被volatile修饰的变量的赋值一旦变化就会通知到其他线程，如果其他线程的工作内存中存在这个变量的拷贝副本，那么其他线程就会放弃这个副本中变量的值，重新去主内存中获取 2.产生了内存屏障，防止指令进行了重排序，关于这点的解释，见如下代码： 123456789public class VolatileTest &#123; int a = 0; //1 int b = 1; //2 volatile int c = 2; //3 int d = 3; //4 int e = 4; //5&#125; 如上所示c变量是被volatile修饰的，那么就会被该段代码产生了一个内存屏障，可以保证在执行语句3的时候，语句1,2是绝对执行完毕的，语句4,5肯定没有执行。上述语句中虽然保证了语句3的执行顺序不可变换，但1,2；4,5语句有可能发生指令重排序。 总结：volatile修饰的变量具有可见性和有序性 12345678910111213141516171819202122232425262728public class VolatileTest &#123;// int a = 0; //1// int b = 1; //2 public static volatile int c = 0; //3// int d = 3; //4// int e = 4; //5 public static void increase()&#123; c++; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; public void run() &#123; increase(); &#125; &#125; ).start(); &#125; Thread.sleep(5000); System.out.println(c); &#125;&#125;//运行3次结果分别是：997，995，989 这个就是典型的volatile操作与原子性的概念。执行结果是小于等于1000的，为什么会这样呢？不是说volatile修饰的变量是具有原子性的么？是的，volatile修饰的变量的确具有原子性，也就是c是具有原子性的（直接赋值是原子性的），但是c++不具有原子性，c++其实就是c = c +1，已经存在了多步操作。所以c具有原子性，但是c++这个操作不具有原子性。 根据前面介绍的java内存模型，当有一个线程去读取主内存的过程中获取c的值，并拷贝一份放入自己的工作内存中，在对c进行+1操作的时候线程阻塞了（各种阻塞情况），那么这个时候有别的线程进入读取c的值，因为有一个线程阻塞就导致该线程无法体现出可见性，导致别的线程的工作内存不会失效，那么它还是从主内存中读取c的值，也会正常的+1操作。如此便导致了结果是小于等于1000的。 注意，这里笔者也有个没有深刻理解的问题，首先在java内存模型中规定了：在对主内存的unlock操作之前必须要执行write操作，那意思就是c在写回之前别的线程是无法读取c的。然而结果却并非如此。如果哪位朋友能理解其中的原委，请与我联系，大家一起讨论研究。 CAS算法CAS的全称叫“Compare And Swap”，也就是比较与交换，他主要的操作思想是： 首先它具有三个操作数，内存位置V，预期值A和新值B，如果在执行过程中，发现内存中的值V与预期值A相匹配，那么他会将V更新为新值A。如果预期值A和内存中的值V不相匹配，那么处理器就不会执行任何操作。CAS算法就是技术点中说的“无锁定算法”，因为线程不必再等待锁定，只要执行CAS操作就可以，会在预期中完成。 如何实现线程安全：我们都知道ConcurrentHashMap核心是线程安全的，那么它又是用什么来实现线程安全的呢？在jdk1.8中主要是采用了CAS算法实现线程安全的。在上一篇博文中已经介绍了CAS的无锁操作，这里不再赘述。同时它通过CAS算法又实现了3种原子操作（线程安全的保障就是操作具有原子性），下面我赋值了源码分别表示哪些成员变量采用了CAS算法，然后又是哪些方法实现了操作的原子性: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283 // Unsafe mechanics CAS保障了哪些成员变量操作是原子性的 private static final sun.misc.Unsafe U; private static final long LOCKSTATE; static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = TreeBin.class; //操作TreeBin,后面会介绍这个类 LOCKSTATE = U.objectFieldOffset (k.getDeclaredField("lockState")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;-------------------------------------------------------------------------------------- private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static &#123; try &#123; //以下变量会在下面介绍到 U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField("sizeCtl")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField("transferIndex")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField("baseCount")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField("cellsBusy")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField("value")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error("data type scale not a power of two"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;//3个原子性操作方法： /* ---------------- Table element access -------------- */ /* * Volatile access methods are used for table elements as well as * elements of in-progress next table while resizing. All uses of * the tab arguments must be null checked by callers. All callers * also paranoically precheck that tab's length is not zero (or an * equivalent check), thus ensuring that any index argument taking * the form of a hash value anded with (length - 1) is a valid * index. Note that, to be correct wrt arbitrary concurrency * errors by users, these checks must operate on local variables, * which accounts for some odd-looking inline assignments below. * Note that calls to setTabAt always occur within locked regions, * and so in principle require only release ordering, not * full volatile semantics, but are currently coded as volatile * writes to be conservative. */ @SuppressWarnings("unchecked") static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); &#125; static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); &#125; 以上这些基本实现了线程安全，还有一点是jdk1.8优化的结果，在以前的ConcurrentHashMap中是锁定了Segment，而在jdk1.8被移除，现在锁定的是一个Node头节点（注意，synchronized锁定的是头结点，这一点从下面的源码中就可以看出来），减小了锁的粒度，性能和冲突都会减少，以下是源码中的体现： 1234567891011121314151617181920212223242526272829303132333435363738//这段代码其实是在扩容阶段对头节点的锁定，其实还有很多地方不一一列举。 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; ..... &#125; &#125; 如何存储数据：知道了ConcurrentHashMap是如何实现线程安全的同时，最起码我们还要知道ConcurrentHashMap又是怎么实现数据存储的。以下是存储的图： 有人看了之后会想，这个不是HashMap的存储结构么？在jdk1.8中取消了segment，所以结构其实和HashMap是极其相似的，在HashMap的基础上实现了线程安全，同时在每一个“桶”中的节点会被锁定。 重要的成员变量：1、capacity：容量，表示目前map的存储大小，在源码中分为默认和最大，默认是在没有指定容量大小的时候会赋予这个值，最大表示当容量达到这个值时，不再支持扩容。 1234567891011121314/** * The largest possible table capacity. This value must be * exactly 1&lt;&lt;30 to stay within Java array allocation and indexing * bounds for power of two table sizes, and is further required * because the top two bits of 32bit hash fields are used for * control purposes. */private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * The default initial table capacity. Must be a power of 2 * (i.e., at least 1) and at most MAXIMUM_CAPACITY. */private static final int DEFAULT_CAPACITY = 16; 2、laodfactor：加载因子，这个和HashMap是一样的，默认值也是0.75f。有不清楚的可以去寻找上篇介绍HashMap的博文。 12345678/** * The load factor for this table. Overrides of this value in * constructors affect only the initial table capacity. The * actual floating point value isn't normally used -- it is * simpler to use expressions such as &#123;@code n - (n &gt;&gt;&gt; 2)&#125; for * the associated resizing threshold. */private static final float LOAD_FACTOR = 0.75f; 3、TREEIFY_THRESHOLD与UNTREEIFY_THRESHOLD：作为了解，这个两个主要是控制链表和红黑树转化的，前者表示大于这个值，需要把链表转换为红黑树，后者表示如果红黑树的节点小于这个值需要重新转化为链表。关于为什么要把链表转化为红黑树，在HashMap的介绍中，我已经详细解释过了。 12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2, and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; 4、下面3个参数作为了解，主要是在扩容和参与扩容（当线程进入put的时候，发现该map正在扩容，那么它会协助扩容）的时候使用，在下一篇博文中会简单介绍到。 12345678910111213141516/** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */private static int RESIZE_STAMP_BITS = 16;/** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;/** * The bit shift for recording size stamp in sizeCtl. */private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; 5、下面2个字段比较重要，是线程判断map当前处于什么阶段。MOVED表示该节点是个forwarding Node，表示有线程处理过了。后者表示判断到这个节点是一个树节点。 12static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of trees12 6、sizeCtl，标志控制符。这个参数非常重要，出现在ConcurrentHashMap的各个阶段，不同的值也表示不同情况和不同功能：①负数代表正在进行初始化或扩容操作②-N 表示有N-1个线程正在进行扩容操作 （前面已经说过了，当线程进行值添加的时候判断到正在扩容，它就会协助扩容）③正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，类似于扩容阈值。它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。实际容量&gt;=sizeCtl，则扩容。 注意：在某些情况下，这个值就相当于HashMap中的threshold阀值。用于控制扩容。 极其重要的几个内部类：如果要理解ConcurrentHashMap的底层，必须要了解它相关联的一些内部类。 1、Node123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Key-value entry. This class is never exported out as a * user-mutable Map.Entry (i.e., one supporting setValue; see * MapEntry below), but can be used for read-only traversals used * in bulk tasks. Subclasses of Node with a negative hash field * are special, and contain null keys and values (but are never * exported). Otherwise, keys and vals are never null. */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; //用volatile修饰 volatile Node&lt;K,V&gt; next;//用volatile修饰 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); //不可以直接setValue &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; 从上面的Node内部类源码可以看出，它的value 和 next是用volatile修饰的，关于volatile已经在前面一篇博文介绍过，使得value和next具有可见性和有序性，从而保证线程安全。同时大家仔细看过代码就会发现setValue（）方法访问是会抛出异常，是禁止用该方法直接设置value值的。同时它还错了一个find的方法，该方法主要是用户寻找某一个节点。 2、TreeNode和TreeBin123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 /** * Nodes for use in TreeBins */ static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; if (k != null) &#123; TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; q; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); &#125; return null; &#125; &#125;//TreeBin太长，笔者截取了它的构造方法： TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root); &#125; 从上面的源码可以看出，在ConcurrentHashMap中不是直接存储TreeNode来实现的，而是用TreeBin来包装TreeNode来实现的。也就是说在实际的ConcurrentHashMap桶中，存放的是TreeBin对象，而不是TreeNode对象。之所以TreeNode继承自Node是为了附带next指针，而这个next指针可以在TreeBin中寻找下一个TreeNode，这里也是与HashMap之间比较大的区别。 3、ForwordingNode123456789101112131415161718192021222324252627282930313233343536/** * A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125;&#125; 这个静态内部内就显得独具匠心，它的使用主要是在扩容阶段，它是链接两个table的节点类，有一个next属性用于指向下一个table，注意要理解这个table，它并不是说有2个table，而是在扩容的时候当线程读取到这个地方发现这个地方为空，这会设置为forwordingNode，或者线程处理完该节点也会设置该节点为forwordingNode，别的线程发现这个forwordingNode会继续向后执行遍历，这样一来就很好的解决了多线程安全的问题。这里有小伙伴就会问，那一个线程开始处理这个节点还没处理完，别的线程进来怎么办，而且这个节点还不是forwordingNode呐？说明你前面没看详细，在处理某个节点（桶里面第一个节点）的时候会对该节点上锁，上面文章中我已经说过了。 认识阶段就写到这里，对这些东西有一定的了解，在下一篇，也就是尾篇中，我会逐字逐句来介绍transfer（）扩容，put（）添加和get（）查询三个方法。 引言transfer方法（扩容方法）再这之前，我大致描述一下扩容的过程：首先有且只能由一个线程构建一个nextTable，这个nextTable主要是扩容后的数组（容量已经扩大），然后把原table复制到nextTable中，这个过程可以多线程共同操作。但是一定要清楚，这个复制并不是简单的把原table的数据直接移动到nextTable中，而是需要有一定的规律和算法操控的（不然怎么把树转化为链表呢）。 再这之前，先简单说下复制的过程：数组中（桶中）总共分为3种存储情况：空，链表头，TreeBin头①遍历原来的数组（原table），如果数组中某个值为空，则直接放置一个forwordingNode（上篇博文介绍过）。②如果数组中某个值不为空，而是一个链表头结点，那么就对这个链表进行拆分为两个链表，存储到nextTable对应的两个位置。③如果数组中某个值不为空，而是一个TreeBin头结点，那么这个地方就存储的是红黑树的结构，这样一来，处理就会变得相对比较复杂，就需要先判断需不需要把树转换为链表，做完一系列的处理，然后把对应的结果存储在nextTable的对应两个位置。 在上一篇博文中介绍过，多个线程进行扩容操作的时候，会判断原table的值，如果这个值是forwordingNode就表示这个节点被处理过了，就直接继续往下找。接下来，我们针对源码逐字逐句介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //stride 主要和CPU相关 //主要是判断CPU处理的量，如果小于16则直接赋值16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating只能有一个线程进行构造nextTable，如果别的线程进入发现不为空就不用构造nextTable了 try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; //把新的数组变为原来的两倍，这里的n&lt;&lt;1就是向左移动一位，也就是乘以二。 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; //原先扩容大小 &#125; int nextn = nextTab.length; //构造一个ForwardingNode用于多线程之间的共同扩容情况 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; //遍历的确认标志 boolean finishing = false; // to ensure sweep before committing nextTab //遍历每个节点 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; //定义一个节点和一个节点状态判断标志fh while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //下面就是一个CAS计算 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; //如果原table已经复制结束 if (finishing) &#123; nextTable = null; //可以看出在扩容的时候nextTable只是类似于一个temp用完会丢掉 table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //修改扩容后的阀值，应该是现在容量的0.75倍 return;//结束循环 &#125; //采用CAS算法更新SizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //CAS算法获取某一个数组的节点，为空就设为forwordingNode else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //如果这个节点的hash值是MOVED，就表示这个节点是forwordingNode节点，就表示这个节点已经被处理过了，直接跳过 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; //对头节点进行加锁，禁止别的线程进入 synchronized (f) &#123; //CAS校验这个节点是否在table对应的i处 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //如果这个节点的确是链表节点 //把链表拆分成两个小列表并存储到nextTable对应的两个位置 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); //CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); //CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; &#125; //如果这个节点是红黑树 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; //如果拆分后的树的节点数量已经少于6个就需要重新转化为链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; //CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); //CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); //CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; PUT方法再这之前，先简单说一下PUT的具体操作：①先传入一个k和v的键值对，不可为空（HashMap是可以为空的），如果为空就直接报错。②接着去判断table是否为空，如果为空就进入初始化阶段。③如果判断数组中某个指定的桶是空的，那就直接把键值对插入到这个桶中作为头节点，而且这个操作不用加锁。④如果这个要插入的桶中的hash值为-1，也就是MOVED状态（也就是这个节点是forwordingNode），那就是说明有线程正在进行扩容操作，那么当前线程就进入协助扩容阶段。⑤需要把数据插入到链表或者树中，如果这个节点是一个链表节点，那么就遍历这个链表，如果发现有相同的key值就更新value值，如果遍历完了都没有发现相同的key值，就需要在链表的尾部插入该数据。插入结束之后判断该链表节点个数是否大于8，如果大于就需要把链表转化为红黑树存储。⑥如果这个节点是一个红黑树节点，那就需要按照树的插入规则进行插入。⑦put结束之后，需要给map已存储的数量+1，在addCount方法中判断是否需要扩容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123;//key和value都不可为空，为空直接抛出错误 if (key == null || value == null) throw new NullPointerException(); //计算Hash值，确定数组下标，这个和HashMap是一样的，我再HashMap的第一篇有介绍过 int hash = spread(key.hashCode()); int binCount = 0; //进入无线循环，直到插入为止 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空或者容量为0就表示没有初始化 if (tab == null || (n = tab.length) == 0) tab = initTable();//初始化数组 //CAS如果查询数组的某个桶是空的，就直接插入该桶中 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin这句话的意思是这个时候插入不用加锁 &#125; //如果在插入的时候，节点是一个forwordingNode状态，表示正在扩容，那么当前线程进行帮助扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; //锁定头节点 synchronized (f) &#123; //确定这个节点的确是数组中的这个头结点 if (tabAt(tab, i) == f) &#123; //是个链表 if (fh &gt;= 0) &#123; binCount = 1; //遍历这个链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果遍历到一个值，这个值和当前的key是相同的，那就更改value值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到结束都没有遇到相同的key，且后面没有节点了，那就直接在尾部插入一个 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果是红黑树存储就需要用红黑树的专门处理了，笔者不再展开。 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //判断节点数量是否大于8，如果大于就需要把链表转化成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //map已存储的数量+1 addCount(1L, binCount); return null;&#125; 其实，相对于transfer来说，PUT理解起来是不是简单很多？说到transfer，咋在PUT方法中都没出现过，只有一个helpTransfer（协助扩容）方法呢？其实，transfer方法放在了addCount方法中，下面是addCount方法的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //是否需要进行扩容操作 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); //如果小于0就说明已经再扩容或者已经在初始化 if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //如果是正在扩容就协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //如果正在初始化就首次发起扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; GET方法Get方法不论是在HashMap和ConcurrentHashMap都是最容易理解的，它的主要步骤是：①先判断数组的桶中的第一个节点是否寻找的对象是为链表还是红黑树，②如果是红黑树另外做处理③如果是链表就先判断头节点是否为要查找的节点，如果不是那么就遍历这个链表查询④如果都不是，那就返回null值。 1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); //数组已被初始化且指定桶中不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //先判断头节点，如果头节点的hash值与入参key的hash值相同 if ((eh = e.hash) == h) &#123; //头节点的key就是传入的key if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val;//返回头节点的value值 &#125; //eh&lt;0表示这个节点是红黑树 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null;//直接从树上进行查找返回结果，不存在就返回null //如果首节点不是查找对象且不是红黑树结构，那边就遍历这个列表 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; //都没有找到就直接返回null值 return null;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>乐观锁</tag>
        <tag>悲观锁</tag>
        <tag>CAS算法</tag>
        <tag>volatile</tag>
        <tag>线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——HashMap]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94HashMap%2F</url>
    <content type="text"><![CDATA[HashMap底层1.数组和链表 数组 存储空间连续，占用内存严重，连续的大内存进入老年代的可能性也会变大，但是正因如此，寻址就显得简单，但是增删时则需要把数据整体往前或往后移动。 链表 存储空间不连续，占用内存较宽松，它的基本结构是一个节点（node）都会包含下一个节点的信息（如果双向链表会存在两个信息一个指向上一个，一个指向下一个），正因为如此寻址就会变得比较困难，插入和删除就显得容易，链表插入和删除的时候只需要修改节点指向信息就可以了。 2.哈希表/散列表（Hash Table非线程安全） 在哈希表的结构中就融入了数组和链表的结构，从而产生了一种寻址容易，插入删除也容易的新存储结构 为了解释方便，我们定义两个东西：String[] arr; 和 List list； 那么，上图左边那一列就是arr, 就是整个的arr[0]~arr[15]，且arr.length() = 16。上图每一行就是一个list，这里理论来说应该最大存储16个List。每个数组存存放的应该是某一个链表的头，也就是arr[0] == list.get(0)。不知道我这样的描述是否清楚。 那么如何确定某一个对象是属于数组的某个下标呢？一般算法就是 下标 = hash(key)%length。算式中的key是存放的对象，hash这个对象会得到一个int值，这个int值就是在上图中所体现的数字，length就是这个数组的长度。我们用上图中的arr[1]打个比方，1%16 =1，337%16 = 1， 353%16 =1。大家就存储在arr[1]中。 HashMap详解1.主要成员变量和方法 loadFactor：称为装载因子，主要控制空间利用率和冲突。大致记住装载因子越大空间利用率更高，但是冲突可能也会变大，反之则相反。源码中默认0.75f。 1234/** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; DEFAULT_INITIAL_CAPACITY与MAXIMUM_CAPACITY：称为容量，用于控制HashMap大小的，下面是源码中的解释： 1234567891011/** * The default initial初始 capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * The maximum capacity, used if a higher value is implicitly隐式 specified指定 * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; THRESHOLD: 这个字段主要是用于当HashMap的size大于它的时候，需要触发resize()方法进行扩容。下面是源码： 12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage.收缩 */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection发现 under removal移动. */static final int UNTREEIFY_THRESHOLD = 6; 看到这里，也许一部分人心中会有一个疑问，为什么前面赋值要用移位的方式，而这里就直接赋值8而不用1&lt;&lt;3呢？注意一个点，在上面有一句解释：“ MUST be a power of two.”，意思就是说这个变量如果发生变化将会是以2的幂次扩容的。比如说1&lt;&lt;4进行扩容一位的话就是1&lt;&lt;4&lt;1那结果就是32啦。 移位算法： 左移位：（低位补0） 例如：5&lt;&lt;2,表示5左移两位 101→10100 右移位：高位负数补1，正数补0 无符号右移：无论正负高位都补0 无论怎么移动，如果移动位数超过规定的bit数，都会与最大移位数取模之后进行计算 例如：int类型是32位的，如果5&lt;&lt;33,其实等价于5&lt;&lt;1 2.红黑树红黑树是自平衡的二叉查找树(有序二叉树)，一般的二叉查找树的时间复杂度为O(lgn)，当一般的二叉查找树退化之后会变成一个线性的列表，这个时候它的时间复杂度就变为了O(n)(这个O(n)其实就是for循环/遍历一次)，但是红黑树它所独有的着色和自平衡的一些性质使得时间复杂度最坏为O(logn)，而不是更低的O(n)，这就是等下我们要说的为什么HashMap在JDK1.8的时候引入冲突解决方案要用红黑树。 从图中就可以看到：①对于任意一个节点，它的左子节点比它小，右子节点比它大， 其实它还有更多的性质。我们就不一一研究了。我们只需要知道这一条性质和红黑树的优势就是可以自平衡。注意，本博客中提到的树性质就是①。 节点插入:遍历树，根据上面描述的性质，只要根据key值的大小可以很快定位到新要插入的节点位置。如果插入破坏了红黑树本身的平衡，红黑树会进行旋转，重新着色进行调整。 节点删除：分为3种情况。①第一要删除的节点处于最外层，那么就可以直接删除。②如果它存在一个子节点，这个子节点直接顶替要删除的节点后并不会破坏整棵树的性质，③如果它存在两个子节点，就先需要拿后继节点来顶替它的位置，在把该节点删除。那小伙伴说什么是后继节点？就上图而言，根节点13的后继节点就是11和15，也就是说节点左边最靠右的，和右边最靠左的。我这么说不知道能否理解呐？删除之后也可能会重新调整树本身的平衡。 3、&amp;与%：因为在HashMap中并不会用%来表达取模运算，而是要用&amp;来运算。也就是一定要记住如下等式： A%B = A&amp;(B-1)，源码中计算桶的位置都是用&amp;来计算的，记住这个，看源码就会轻松一些。 4.红黑树源码12345678910static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; //父节点 TreeNode&lt;K,V&gt; left;//左子节点 TreeNode&lt;K,V&gt; right;//右子节点 TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; //节点颜色 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; 5.核心hashMap123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178//reszie()方法： final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; //定义旧表并把原本的赋值给他，从昨天的博文中可以知道new了一个HashMap对象之后其实table是为null的。 int oldCap = (oldTab == null) ? 0 : oldTab.length;//如果旧表容量为null就初始0 int oldThr = threshold;//旧表的阀值 int newCap, newThr = 0;//定义新表的容量和新表的阀值 //进入条件：正常扩容 if (oldCap &gt; 0) &#123;如果旧表容量大于0，这个情况就是要扩容了 //进入条件：已达到最大，无法扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果容量已经大于等于1&lt;&lt;30 threshold = Integer.MAX_VALUE;//设置阀值最大 return oldTab;//直接返回原本的对象(无法扩大了) &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY)//旧表容量左移一位&lt;&lt;1,且移动之后处于合法的范围之中。新表容量扩充完成 newThr = oldThr &lt;&lt; 1; // 新表的阀值也扩大一倍。 &#125; //进入条件：初始化的时候使用了自定义加载因子的构造函数 else if (oldThr &gt; 0) // 这里如果执行的情况是原表容量为0的时候，但是阀值又不为0。hashmap的构造函数不同（需要设置自己的加载因子）的时候会触发。 newCap = oldThr; //进入条件：调用无参或者一个参数的构造函数进入默认初始化 else &#123; // 如果HashMap默认构造就会进入下面这个初始化，我们昨天（上一篇博文）的第一次put就会进入下面这一块。 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//初始化完成。 &#125; //进入条件：初始化的时候使用了自定义加载因子的构造函数 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor;//新表容量*加载因子 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;//确定新的阀值 //开始构造新表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //进入条件：原表存在 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123;//开始遍历 oldTab[j] = null;//旧表原本置空 if (e.next == null)//不存在下个节点，也就是目前put的就是链表头 newTab[e.hash &amp; (newCap - 1)] = e;把该对象赋值给新表的某一个桶中 //进入条件：判断桶中是否已红黑树存储的。如果是红黑树存储需要宁做判断 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //进入条件：如果桶中的值是合法的，也就是不止存在一个，也没有触发红黑树存储 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next;//获取下一个对象信息 //因为桶已经扩容了两倍，所以以下部分是按一定逻辑的把一个链表拆分为两个链表，放入对应桶中。具体的拆分流程，各位看官们仔细研究下。笔者已经看得头晕了。 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125;//put方法： public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125;//putVal()方法： final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //进入条件:HashMap初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //进入条件:如果计算得到的桶中不存在别的对象 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null);//直接把目前对象赋值给当前空对象中 else &#123; Node&lt;K,V&gt; e; K k; //进入条件：判断桶中第一个是否有相同的key值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //进入条件：如果该桶中的对象已经由红黑树构造了就特别处理 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; //进入条件：判断当前桶中对象存储是否达到8个，如果达到了就进入treeifyBin方法，这里不再进入方法详解。大致就是进入之后再判断当前hashMap的长度，如果不足64，只进行扩容，如果达到64就需要构建红黑树了。 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //进入条件：遍历中查看是否有相同的key值。如果有直接结束遍历并赋值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //进入条件：如果链表上有相同的key值。进行替换 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null)//这句笔者理解半天也未有所获，如果谁能清楚知道可以联系笔者一起探讨 e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //如果hashMap的大小大于阀值就需要扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125;//get()方法： public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125;getNode()方法： final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //进入条件：如果HashMap不为空，且求得的数组下标那个对象不为空（关于%和&amp;的关系上面方法已有讲过） if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //先判断头结点是否是要找的值 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //然后再进入链表遍历 if ((e = first.next) != null) &#123; if (first instanceof TreeNode)//如果头链表已经是红黑树构造就需要用红黑树的方法去遍历 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; //直到找到拥有相同key的时候返回 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>数组</tag>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——堆]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%A0%86%2F</url>
    <content type="text"><![CDATA[引言堆，我们一般作为二叉堆的一种总称，它是建立在二叉树之上的。在本篇中，会详细介绍堆的结构和原理，以至于写出堆的实现。在代码实现中我们主要是针对于插入和删除做一些操作，在删除中我们只考虑删除最小的，而不涉及更深一步的操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Example&lt;T&gt; &#123; //定义一个头节点 Node head=null; //定义一个内部类，表示一个链表的节点 class Node&#123; private T t; Node next; public Node(T t)&#123; this.t=t; &#125; &#125; //链表插入数据 public void insert(T t)&#123; Node node = new Node(t);//node address=a1,t=1;node address=a2,t=2;node address=a3,t=3 //如果头结点是空，那就是首次插入 if(head==null)&#123; head=node;//head=node=a1 &#125;else &#123; //如果头结点不为空，那么寻找最后为空的位置插入 Node p=head;//p=a1;p=a1 while(p.next!=null)&#123; p=p.next;//p=a2 &#125; p.next=node;//p.next=a2;p.next=a3 &#125; &#125; //展示链表状态 public void print()&#123; Node p=head;//p=a1 while(p!=null)&#123; System.out.print(p.t+"-&gt;"); p=p.next; &#125; System.out.println("null\n"); &#125; public static void main(String[] args) &#123; //构建一个链表 Example&lt;Integer&gt; example = new Example&lt;&gt;(); example.insert(1); example.insert(2); example.insert(3); example.insert(4); example.insert(5); example.print(); &#125;&#125;//1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null 场景引入我们在考虑优先队列的时候会有这样的场景：比如说整个公司都用同一台打印机。一般说来会有队列实现，它遵循FIFO的规则，先提交打印任务的先打印，这无可厚非。但是在实际中，我们希望重要的文件先打印，比如说甲有50页不重要的文件和乙有2页重要文件，甲先提交，这种情况下，我们希望乙能够先打印。FIFO的规则显然不合适。继续讨论这个问题，如果我们用自定义的链表实现呢？这里可以分为两种情况：①如果链表是有序的，那么删除最小的元素的时间复杂度是O(1)，但是插入的时间复杂度就是O(N)。②如果链表是无序的，那么插入定义为插入到最尾部，那么时间复杂度是O(1)，但是删除最小的元素时间复杂度就是O(N)。继续深究一下，如果我用二叉查找树呢？按照二叉查找树的性质来说，我们插入和删除最小元素的时间复杂度都是O(Log N)，相比于链表来说有一定的优化，但是我们要考虑一个问题，频繁的删除最小节点，会导致二叉查找树的退化，也就是说二叉查找树的右子树会比左子树大的多，也有可能会直接退化成链表。 完全二叉树通俗来说，在除最后一层外，每一层上的节点数均达到最大值，在最后一层上只缺少右边的若干结点。大家可以看下面这张图理解：再说明一下，只能缺少右边的若干节点，并不是可以缺少右子节点。 二叉堆堆是一颗被完全填满的二叉树，如果没有填满，那么只能是在最后一层，而且在这层上，所有的元素从左到右填入，如果有空缺，只能空缺右边的元素。通俗来说它就是一颗完全二叉树。同时它分为两类描述：①最小堆意思就是最小的元素在堆顶，且每一个父节点的值都要小于子节点，下图就是一个最小堆：②最大堆意思就是最大的在堆顶，且每一个父节点的值都大于子节点，下图就是一个最大堆：我们在代码实现过程中，已最小堆为例。 代码实现1、描述方式我们思考二叉堆，发现他不需要用链表来表述，直接用数组就可以表述了，我们尝试把二叉堆从上至下，一层一层平铺成数组。我们把上面的最小堆用数组表示就是： 我们对于其进行描述，对于一个二叉堆平铺之后的数组，我们可以发现，任意一个下标元素arr[i]，他的左孩子的就是arr[2i]，他的右孩子就是arr[2i+1]，他的父节点就是arr[i/2]。 为什么可以用数组来表述二叉堆？因为完全二叉树的性质，只能在最后一层的右侧允许缺少节点，而这些节点在数组中处于连续的末端，并不影响前面的所有元素。 2、插入二叉堆的插入还是很有意思的，一般，我们采用上滤的方式来解决二叉堆的插人：①确认一个可以插入的预插入位置，如果最后一层不满的话，那就插入到最后一层最靠左的那个位置，如果最后一层已满的话，那就新开一层，插入到最左边；②判断把当前数据放入到这个位置是否会破坏二叉堆的性质，如果不破坏，那么插入结束；③如果不符合，那么就需要把这个预插入位置和它的父节点进行兑换；重复②③步骤，直至插入结束。下面这张图描述了这种插入过程： 3、删除理解了插入的过程，删除其实也不难的。想对应的，我们称这种方法为下滤。在最小堆中，我们知道如果要删除最小的，那么其实就是删除堆顶就可以了。可想而知，那我们删除之后，有必要把整个二叉堆恢复到满足的条件。也就是说：①移除堆顶元素。并指定当前位置为预插入位置，并尝试把最后一个元素（最后一个元素在二叉堆的最后一层的最后一个位置）放到这个②如果不能顺利插入，那么就比较它的孩子，把较小的孩子放入这个预插入位置。③继续处理这个预插入位置，循环②步骤，直至又形成一个完整的二叉堆位置。下面这张图描述了这种删除最小的过程： 代码实现以下是用代码实现的二叉堆，包含了初始化，插入和删除： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139public class Heap&lt;T extends Comparable&lt;? super T&gt;&gt; &#123; private static final int DEFAULT_CAPACITY = 10; //默认容量 private T[] table; //用数组存储二叉堆 private int size; //表示当前二叉堆中有多少数据 public Heap(int capactiy)&#123; this.size = 0;//初始化二叉堆数据量 table = (T[]) new Comparable[capactiy + 1];//+1是因为我们要空出下标为0的元素不存储 &#125; public Heap() &#123;//显得专业些，你就要定义好构造器 this(DEFAULT_CAPACITY); &#125; //插入 public void insert(T t)&#123; //先判断是否需要扩容 if(size == table.length - 1)&#123; resize(); &#125; //开始插入 //定义一个预插入位置下标 int target = ++size; //循环比较父节点进行位置交换 for(table[ 0 ] = t; t.compareTo(table[target/2]) &lt; 0; target /= 2)&#123; table[target] = table[target/2];//如果满足条件，那么两者交换，直到找到合适位置（上滤） &#125; //插入数据 table[target] = t; print(); &#125; //删除最小 //删除过程中，需要重新调整二叉堆（下滤） public void deleteMin()&#123; if(size == 0)&#123; throw new IllegalAccessError("二叉堆为空"); &#125; //删除元素 table[1] = table[size--]; int target = 1;//从顶部开始重新调整二叉堆 int child;//要处理的节点下标 T tmp = table[ target ]; for( ; target * 2 &lt;= size; target = child ) &#123; child = target * 2; if( child != size &amp;&amp;table[ child + 1 ].compareTo( table[ child ] ) &lt; 0 )&#123;//如果右孩子比左孩子小 child++; &#125; if( table[ child ].compareTo( tmp ) &lt; 0 )&#123; table[ target ] = table[ child ]; table[child] = null; &#125; else&#123; break; &#125; &#125; table[ target ] = tmp; print(); &#125; //如果插入数据导致达到数组上限，那么就需要扩容 private void resize()&#123; T [] old = table; table = (T []) new Comparable[old.length*2 + 1];//把原来的数组扩大两倍 for( int i = 0; i &lt; old.length; i++ ) table[ i ] = old[ i ]; //数组进行拷贝 &#125; //打印数组 private void print()&#123; System.out.println(); for (int i = 1; i &lt;= size; i++) &#123; System.out.print(table[i] + " "); &#125; System.out.println("二叉堆大小:"+size); &#125; public static void main(String[] args) &#123; Heap&lt;Integer&gt; heap = new Heap&lt;&gt;(); //循环插入0~9的数据 for (int i = 0; i &lt; 10; i++) &#123; heap.insert(i); &#125; //循环删除3次，理论上是删除0,1,2 for (int i = 0; i &lt; 3; i++) &#123; heap.deleteMin(); &#125; &#125;&#125;//输出结果：////0 二叉堆大小:1////0 1 二叉堆大小:2////0 1 2 二叉堆大小:3////0 1 2 3 二叉堆大小:4////0 1 2 3 4 二叉堆大小:5////0 1 2 3 4 5 二叉堆大小:6////0 1 2 3 4 5 6 二叉堆大小:7////0 1 2 3 4 5 6 7 二叉堆大小:8////0 1 2 3 4 5 6 7 8 二叉堆大小:9////0 1 2 3 4 5 6 7 8 9 二叉堆大小:10////1 3 2 7 4 5 6 9 8 二叉堆大小:9////2 3 5 7 4 8 6 9 二叉堆大小:8////3 4 5 7 9 8 6 二叉堆大小:7 尾记这里，对于新手来说有一个小小的规则。对于一个类，代码模块存放顺序一般都是：静态成员变量/常量，构造方法，public方法，private方法。我主要说的是要把你封装起来的private方法放到最后面，因为别人查看你的代码的时候，别人希望最先看到的是你暴露出来的public方法，而不是对他来说无关紧要的private方法。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
</search>
