<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[算法——排序]]></title>
    <url>%2F2018%2F11%2F05%2F%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1.插入排序1234567891011121314public static void sort(int[] n)&#123; int k; for (int i = 1; i &lt;n.length ; i++) &#123;//遍历数组，从1下标开始 //用temp暂存要排序的值 int temp=n[i]; //待处理的数据，在已排好的数组中从后往前进行比较遍历 //条件是：待处理数据是否比有序的数据中某值小，如果小就要数组进行移动 for ( k = i; k &gt;0&amp;&amp;temp&lt;n[k-1] ; k--) &#123; //数组移位，位数据的插入做准备 n[k]=n[k-1]; &#125; n[k]=temp; &#125; &#125; 2.快速排序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class QuickSort &#123; //最容易理解的快速排序 //我们以int排序为例，方便起见，就不像以前一样实现对象的compareTo方法了 public static void sort(List&lt;Integer&gt; list)&#123; //递归结束条件 if(list.size() &gt; 1)&#123;//上面说的，当数据量为1或者0的时候结束递归 //建立三个集合，分别表示小于枢纽元，大于枢纽元和等于枢纽元 List&lt;Integer&gt; smallerList = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; largerList = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; sameList = new ArrayList&lt;Integer&gt;(); //选取一个随机值作为枢纽元，在我们学习过程中，我们通常把第一个数作为枢纽元 Integer pivot = list.get(0); //遍历list，把比pivot小的放smallerList中，比pivot大的放largerList中,相等的放sameList中 for (Integer integer : list) &#123; if(integer &lt; pivot)&#123; smallerList.add(integer); &#125; else if(integer &gt; pivot)&#123; largerList.add(integer); &#125;else&#123; sameList.add(integer); &#125; &#125; //递归实现分组后的子数据进行上面同样的操作 sort(smallerList); sort(largerList); //对排序好的数据进行整合 list.clear();//清楚原本的数据，用于存放有序的数据 list.addAll(smallerList); list.addAll(sameList); list.addAll(largerList); &#125; &#125; public static void main(String[] args) &#123; Integer[] target = &#123;4,3,6,7,1,9,5,2,3,3&#125;; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; target.length; i++) &#123; list.add(target[i]); &#125; sort(list); //查看排序结果 for (Integer integer : list) &#123; System.out.print(integer + " "); &#125; &#125;&#125;//输出结果：//1 2 3 3 3 4 5 6 7 9 // 为什么我要用一个List来存储一样大小的呢？ 这里就暴露出了快速排序算法中对相同数据的处理方式。在上述的实现中为什么不直接把相等的放入smallerList或者放入largerList呢？举个例子：假如说你选取的枢纽元是最小值，那么是不是可能发生每次递归的数据都是一样的？因为所有的数据都比你的枢纽元大，而且这个时候恰好你把相等的数据都放入了比枢纽元大的部分，那么就会造成栈溢出了。所以在这个过程中，我们需要对相等的数据单独存储起来。 如何选取枢纽元1、以第一个值为枢纽元在大学期间，我们学习到快速排序，告知我们的一般都是以第一个值为枢纽元。对于这种默认的选取方式，我们对他进行剖析：①如果待排序的数据是随机的，那么如此选择枢纽元是可以接受的，因为在概率上来说，随机的情况下，在第一次快速排序之后，会分为两个差不多相等的新的数据。②如果待排序的数据是有序的，那么这种情况下，就不能以第一个值为枢纽元了，因为它会产生一种恶心分割，直接导致所有的元素都被划分到左边子数据或者右边子数据。所以这种办法是不可取的，也尽量不去实现它。也有人说可以选取第一个和第二个数据做比较，比较大的作为枢纽元。这种方式只是简单的规避了划分为空的情况，这种恶心的划分还是存在的。 2、随机选择一个枢纽元在待排序的数据中随机选取一个数据为枢纽元会显得安全很多。它的随机可以保证在分割的过程中可以合理、平均的进行划分。但是我们要考虑随机数产生的开销，每趟分割之前还需要随机出一个随机数，那么开销会变得非常巨大。所以这种方式虽然比较安全，但是性能仍旧是不可取的。 3、三数中值分割法三数中值分割法是目前比较高效的一种选取枢纽元的方式。按照选取枢纽元的要求：要尽可能合理、平均的划分为两部分。那么最好的是选取一个中值，那么就可以精准的分割两部分了。但是我们不可能划分这个开销去寻找中值，我们做的只是:我们从待排序的数据中选取3个位置的值，分别是第一位置、最后位置、中间位置。然后我们用这3个数据中排序中间的数据作为枢纽元。 我们在选取好枢纽元之后仍旧需要遍历3个之中剩余的两个值，因为这里已经比较了一轮，我们只需要在枢纽元选取的时候就把剩余两个进行排序了，比枢纽元小的放在最前面，比枢纽元大的放在最后面，且在遍历的时候跳过这两个值。所以说三数中值分割法并没有白白花费这个效率开销。 我们要对数据：3，2，5，7，1，8，9进行快速排序。我们随机选取一个值为枢纽元：5，那么我们就把5与9进行位置交换，把枢纽元独立到数据的边缘，避免它参与数据交换，所以初始情况就是这样：3，2，9，7，1，8，5我们定义两个指针，这两个指针我们称为头指针和尾指针，分别指向第一个元素和倒数第二个元素（倒数第一个元素是枢纽元）。接着就需要开始移动指针：在头指针指向的位置小于尾指针指向的位置时：①我们将头指针向右移动，遇到比枢纽元小的元素直接移动到下一个数据，直到遇到一个数据大于枢纽元，则头指针停止运动。②相同的，移动尾指针向前移动，遇到比枢纽大的元素直接移动到下一个数据，直到遇到一个数据小于枢纽元。③等两个指针都停止下来的时候，就需要把两个指针所指向的数据进行交换。并继续重复①②③步骤。④直到尾指针指向的位置小于头指针指向的位置，通俗来说就是两个指针交错了就结束遍历。 下面是对于上面数据快速排序的一趟图解: 在此基础上，我们再来说说为什么三数中值分割法并没有浪费额外的开销：用三数中值分割法获取到枢纽元，那么其实比这个枢纽元小的数据可以放在最左边，比这个枢纽元大的数据放在最右边。那么这样一来，头指针就可以从第二个开始，尾指针就可以从倒数第二个开始，这样一来，一定程度上效率是有回升的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class QuickSort &#123; //暴露给外部的接口，对一个数组进行排序 public static void quicksort(Integer [] target)&#123; quicksoort(target, 0, target.length - 1); &#125; //具体实现 //用left与right的方式，尽可能的实现数组复用 private static void quicksoort(Integer[] target, int left, int right)&#123; if(left + 2 &lt; right)&#123;//递归结束条件,之所以+2是因为三数中值分割法最起码需要两个数据 //寻找枢纽元 int pivot = findPivot(target, left, right, false); //定义头指针与尾指针 int i = left + 1, j = right - 2; //因为三数中值分割法导致最前数据和最后数据不用判断 for( ; ; )&#123; //两个指针开始运动，直到两者都停止 while(target[i] &lt;= pivot)&#123;i++;&#125;//如果头指针遍历到小于枢纽元的数据直接跳过 while(target[j] &gt;= pivot)&#123;j--;&#125;//如果尾指针遍历到大于枢纽元的数据直接跳过 //判断两个指针是否交错 if(i &lt; j)&#123; //没有交错，且指针停止，那么进行数据交换 swap(target, i, j); &#125;else&#123; break;//指针交错，那么结束循环 &#125; &#125; //也就是上面描述的指针交错之后，需要把枢纽元交换到头指针的位置 swap(target, i, right - 1); //继续递归子数组 quicksoort(target, left, i - 1); quicksoort(target, i + 1, right); &#125;else&#123; //当数据少于2个的时候，直接用三数中值分割法进行排序 findPivot(target, left, right, true); &#125; &#125; //三数中值分割法 //这个判断用于说明是否最后的操作，最后的操作不需要把枢纽值放到最后 private static Integer findPivot(Integer[] target, int left, int right, boolean end)&#123; int mid = (left + right) / 2;//获取中间值的位置 //比较开始数据与中间数据 if(target[left] &gt; target[mid])&#123; //如果开始数据比中间数据大，那么位置进行交换 swap(target, left, mid); &#125; if(target[left] &gt; target[right])&#123; //如果开始的数据比最后数据大，那么交换位置 swap(target, left, right); &#125; if(target[mid] &gt; target[right])&#123; //如果中间的数据比最后的数据大，那么交换位置 swap(target, mid, right); &#125; if(!end)&#123; //按照前面说的，把枢纽元放到最后面 swap(target, mid, right - 1); //返回枢纽元 return target[right - 1]; &#125; return null; &#125; //交换数组中两个下标的数据 private static final void swap(Integer[] target, int one, int anthor)&#123; int temp = target[one]; target[one] = target[anthor]; target[anthor] = temp; &#125; public static void main(String[] args) &#123; Integer[] target = &#123;4,3,6,7,1,9,5,2,3,3&#125;; quicksort(target); for (Integer integer : target) &#123; System.out.print(integer + " "); &#125; &#125;&#125;//输出结果：//1 2 3 3 3 4 5 6 7 9 // 各种排序算法性能分析：]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>插入排序</tag>
        <tag>快速排序</tag>
        <tag>排序算法性能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——ConcurrentHashMap]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94ConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[技术点：1.悲观锁和乐观锁悲观锁是指如果一个进程占用了一个锁，而导致其他需要这个锁的线程进入等待，一直到该锁被释放，换句话说就是这个锁被独占，比如典型的synchronized； 乐观锁是指操作并不加锁，而是抱着尝试的心态去执行某项操作，如果操作失败或者操作冲突，那么就进入重试，一直到执行成功为止。 2.原子性，指令有序性和线程可见性这三个性质是多线程编程中核心的问题。 原子性和事务的原子性一样，对于一个或多个操作，要么都执行，要么都不执行。指令有序性是指，在我们编写的代码中，上下两个互不关联的语句不会被指令重排序。 指令重排序是指处理器为了性能优化，在无关联的代码的执行是可能会和代码顺序不一致。比如说int i=1;int j=2;那么这两条语句的执行顺序可能会先执行int j=2； 线程可见性是指一个线程修改了某个变量，其他线程马上能知道。 3.无锁算法使用低层原子化的机器指令，保证并发情况下数据的完整性，典型的如CAS算法。 4.内存屏障在《深入理解JVM》中解释：它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障之后；即在执行到内存屏障这句指令时，它前面的操作已经全部完成；它会强制对缓存的修改操作立即写入主存；如果是写操作，它会导致其他CPU中对应的缓存行无效。在使用volatile修饰的变量会产生内存屏障。 Java内存模型 1.解释：每个线程会从主内存中读取操作，所有的变量都存储在主内存中，每个线程都需要从主内存中获得变量的值。 然后如图可见，每个线程获得数据之后会放入自己的工作内存，这就是java内存模型的规定之二，保证每个线程操作的都是从主内存拷贝的副本，也就是说线程不能直接操作主内存中的变量，需要把主内存的变量值读取之后放入自己的工作内存中的变量副本中，然后操作这个副本。 最后线程和线程之间无法直接访问对方工作内存中的变量。最后需要解释一下这个访问规则局限于对象实例字段，静态字段等，局部变量不包括在内，因为局部变量不存在竞争问题。 2.基本执行步骤：a.lock(锁定)：在某一个线程在读取主内存的时候需要把变量锁定 b.unlock（解锁）：在某一个线程读取完变量值之后会释放锁定，别的线程就可以进入操作 c.read（读取）：从主内存中读取变量的值并放入工作内存中 d.load（加载）：从read操作中得到的值放入工作内存变量副本中 e.use（使用）：把工作内存中的一个变量值传递给执行引擎 f.assign（赋值）：它把一个从执行引擎接收到的值赋值给工作内存的变量 g.store（存储）：把工作内存中的一个变量的值传送到主内存中 h.write（写入）：把store操作从工作内存中的一个变量的值传送到主内存的变量中 volatile关键字在concurrentHashMap中，有很多成员变量都是用volatile修饰的，被volatile修饰的变量有如下特性： 1.使得变量更新变得具有可见性，只有被volatile修饰的变量的赋值一旦变化就会通知到其他线程，如果其他线程的工作内存中存在这个变量的拷贝副本，那么其他线程就会放弃这个副本中变量的值，重新去主内存中获取 2.产生了内存屏障，防止指令进行了重排序，关于这点的解释，见如下代码： 123456789public class VolatileTest &#123; int a = 0; //1 int b = 1; //2 volatile int c = 2; //3 int d = 3; //4 int e = 4; //5&#125; 如上所示c变量是被volatile修饰的，那么就会被该段代码产生了一个内存屏障，可以保证在执行语句3的时候，语句1,2是绝对执行完毕的，语句4,5肯定没有执行。上述语句中虽然保证了语句3的执行顺序不可变换，但1,2；4,5语句有可能发生指令重排序。 总结：volatile修饰的变量具有可见性和有序性 12345678910111213141516171819202122232425262728public class VolatileTest &#123;// int a = 0; //1// int b = 1; //2 public static volatile int c = 0; //3// int d = 3; //4// int e = 4; //5 public static void increase()&#123; c++; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; public void run() &#123; increase(); &#125; &#125; ).start(); &#125; Thread.sleep(5000); System.out.println(c); &#125;&#125;//运行3次结果分别是：997，995，989 这个就是典型的volatile操作与原子性的概念。执行结果是小于等于1000的，为什么会这样呢？不是说volatile修饰的变量是具有原子性的么？是的，volatile修饰的变量的确具有原子性，也就是c是具有原子性的（直接赋值是原子性的），但是c++不具有原子性，c++其实就是c = c +1，已经存在了多步操作。所以c具有原子性，但是c++这个操作不具有原子性。 根据前面介绍的java内存模型，当有一个线程去读取主内存的过程中获取c的值，并拷贝一份放入自己的工作内存中，在对c进行+1操作的时候线程阻塞了（各种阻塞情况），那么这个时候有别的线程进入读取c的值，因为有一个线程阻塞就导致该线程无法体现出可见性，导致别的线程的工作内存不会失效，那么它还是从主内存中读取c的值，也会正常的+1操作。如此便导致了结果是小于等于1000的。 注意，这里笔者也有个没有深刻理解的问题，首先在java内存模型中规定了：在对主内存的unlock操作之前必须要执行write操作，那意思就是c在写回之前别的线程是无法读取c的。然而结果却并非如此。如果哪位朋友能理解其中的原委，请与我联系，大家一起讨论研究。 CAS算法CAS的全称叫“Compare And Swap”，也就是比较与交换，他主要的操作思想是： 首先它具有三个操作数，内存位置V，预期值A和新值B，如果在执行过程中，发现内存中的值V与预期值A相匹配，那么他会将V更新为新值A。如果预期值A和内存中的值V不相匹配，那么处理器就不会执行任何操作。CAS算法就是技术点中说的“无锁定算法”，因为线程不必再等待锁定，只要执行CAS操作就可以，会在预期中完成。 如何实现线程安全：我们都知道ConcurrentHashMap核心是线程安全的，那么它又是用什么来实现线程安全的呢？在jdk1.8中主要是采用了CAS算法实现线程安全的。在上一篇博文中已经介绍了CAS的无锁操作，这里不再赘述。同时它通过CAS算法又实现了3种原子操作（线程安全的保障就是操作具有原子性），下面我赋值了源码分别表示哪些成员变量采用了CAS算法，然后又是哪些方法实现了操作的原子性: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283 // Unsafe mechanics CAS保障了哪些成员变量操作是原子性的 private static final sun.misc.Unsafe U; private static final long LOCKSTATE; static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = TreeBin.class; //操作TreeBin,后面会介绍这个类 LOCKSTATE = U.objectFieldOffset (k.getDeclaredField("lockState")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;-------------------------------------------------------------------------------------- private static final sun.misc.Unsafe U; private static final long SIZECTL; private static final long TRANSFERINDEX; private static final long BASECOUNT; private static final long CELLSBUSY; private static final long CELLVALUE; private static final long ABASE; private static final int ASHIFT; static &#123; try &#123; //以下变量会在下面介绍到 U = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentHashMap.class; SIZECTL = U.objectFieldOffset (k.getDeclaredField("sizeCtl")); TRANSFERINDEX = U.objectFieldOffset (k.getDeclaredField("transferIndex")); BASECOUNT = U.objectFieldOffset (k.getDeclaredField("baseCount")); CELLSBUSY = U.objectFieldOffset (k.getDeclaredField("cellsBusy")); Class&lt;?&gt; ck = CounterCell.class; CELLVALUE = U.objectFieldOffset (ck.getDeclaredField("value")); Class&lt;?&gt; ak = Node[].class; ABASE = U.arrayBaseOffset(ak); int scale = U.arrayIndexScale(ak); if ((scale &amp; (scale - 1)) != 0) throw new Error("data type scale not a power of two"); ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;//3个原子性操作方法： /* ---------------- Table element access -------------- */ /* * Volatile access methods are used for table elements as well as * elements of in-progress next table while resizing. All uses of * the tab arguments must be null checked by callers. All callers * also paranoically precheck that tab's length is not zero (or an * equivalent check), thus ensuring that any index argument taking * the form of a hash value anded with (length - 1) is a valid * index. Note that, to be correct wrt arbitrary concurrency * errors by users, these checks must operate on local variables, * which accounts for some odd-looking inline assignments below. * Note that calls to setTabAt always occur within locked regions, * and so in principle require only release ordering, not * full volatile semantics, but are currently coded as volatile * writes to be conservative. */ @SuppressWarnings("unchecked") static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); &#125; static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); &#125; 以上这些基本实现了线程安全，还有一点是jdk1.8优化的结果，在以前的ConcurrentHashMap中是锁定了Segment，而在jdk1.8被移除，现在锁定的是一个Node头节点（注意，synchronized锁定的是头结点，这一点从下面的源码中就可以看出来），减小了锁的粒度，性能和冲突都会减少，以下是源码中的体现： 1234567891011121314151617181920212223242526272829303132333435363738//这段代码其实是在扩容阶段对头节点的锁定，其实还有很多地方不一一列举。 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; ..... &#125; &#125; 如何存储数据：知道了ConcurrentHashMap是如何实现线程安全的同时，最起码我们还要知道ConcurrentHashMap又是怎么实现数据存储的。以下是存储的图： 有人看了之后会想，这个不是HashMap的存储结构么？在jdk1.8中取消了segment，所以结构其实和HashMap是极其相似的，在HashMap的基础上实现了线程安全，同时在每一个“桶”中的节点会被锁定。 重要的成员变量：1、capacity：容量，表示目前map的存储大小，在源码中分为默认和最大，默认是在没有指定容量大小的时候会赋予这个值，最大表示当容量达到这个值时，不再支持扩容。 1234567891011121314/** * The largest possible table capacity. This value must be * exactly 1&lt;&lt;30 to stay within Java array allocation and indexing * bounds for power of two table sizes, and is further required * because the top two bits of 32bit hash fields are used for * control purposes. */private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * The default initial table capacity. Must be a power of 2 * (i.e., at least 1) and at most MAXIMUM_CAPACITY. */private static final int DEFAULT_CAPACITY = 16; 2、laodfactor：加载因子，这个和HashMap是一样的，默认值也是0.75f。有不清楚的可以去寻找上篇介绍HashMap的博文。 12345678/** * The load factor for this table. Overrides of this value in * constructors affect only the initial table capacity. The * actual floating point value isn't normally used -- it is * simpler to use expressions such as &#123;@code n - (n &gt;&gt;&gt; 2)&#125; for * the associated resizing threshold. */private static final float LOAD_FACTOR = 0.75f; 3、TREEIFY_THRESHOLD与UNTREEIFY_THRESHOLD：作为了解，这个两个主要是控制链表和红黑树转化的，前者表示大于这个值，需要把链表转换为红黑树，后者表示如果红黑树的节点小于这个值需要重新转化为链表。关于为什么要把链表转化为红黑树，在HashMap的介绍中，我已经详细解释过了。 12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2, and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6; 4、下面3个参数作为了解，主要是在扩容和参与扩容（当线程进入put的时候，发现该map正在扩容，那么它会协助扩容）的时候使用，在下一篇博文中会简单介绍到。 12345678910111213141516/** * The number of bits used for generation stamp in sizeCtl. * Must be at least 6 for 32bit arrays. */private static int RESIZE_STAMP_BITS = 16;/** * The maximum number of threads that can help resize. * Must fit in 32 - RESIZE_STAMP_BITS bits. */private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;/** * The bit shift for recording size stamp in sizeCtl. */private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; 5、下面2个字段比较重要，是线程判断map当前处于什么阶段。MOVED表示该节点是个forwarding Node，表示有线程处理过了。后者表示判断到这个节点是一个树节点。 12static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of trees12 6、sizeCtl，标志控制符。这个参数非常重要，出现在ConcurrentHashMap的各个阶段，不同的值也表示不同情况和不同功能：①负数代表正在进行初始化或扩容操作②-N 表示有N-1个线程正在进行扩容操作 （前面已经说过了，当线程进行值添加的时候判断到正在扩容，它就会协助扩容）③正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，类似于扩容阈值。它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。实际容量&gt;=sizeCtl，则扩容。 注意：在某些情况下，这个值就相当于HashMap中的threshold阀值。用于控制扩容。 极其重要的几个内部类：如果要理解ConcurrentHashMap的底层，必须要了解它相关联的一些内部类。 1、Node123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Key-value entry. This class is never exported out as a * user-mutable Map.Entry (i.e., one supporting setValue; see * MapEntry below), but can be used for read-only traversals used * in bulk tasks. Subclasses of Node with a negative hash field * are special, and contain null keys and values (but are never * exported). Otherwise, keys and vals are never null. */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; //用volatile修饰 volatile Node&lt;K,V&gt; next;//用volatile修饰 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; return key + "=" + val; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); //不可以直接setValue &#125; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((o instanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125; 从上面的Node内部类源码可以看出，它的value 和 next是用volatile修饰的，关于volatile已经在前面一篇博文介绍过，使得value和next具有可见性和有序性，从而保证线程安全。同时大家仔细看过代码就会发现setValue（）方法访问是会抛出异常，是禁止用该方法直接设置value值的。同时它还错了一个find的方法，该方法主要是用户寻找某一个节点。 2、TreeNode和TreeBin123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100 /** * Nodes for use in TreeBins */ static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; if (k != null) &#123; TreeNode&lt;K,V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K,V&gt; q; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.findTreeNode(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); &#125; return null; &#125; &#125;//TreeBin太长，笔者截取了它的构造方法： TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root); &#125; 从上面的源码可以看出，在ConcurrentHashMap中不是直接存储TreeNode来实现的，而是用TreeBin来包装TreeNode来实现的。也就是说在实际的ConcurrentHashMap桶中，存放的是TreeBin对象，而不是TreeNode对象。之所以TreeNode继承自Node是为了附带next指针，而这个next指针可以在TreeBin中寻找下一个TreeNode，这里也是与HashMap之间比较大的区别。 3、ForwordingNode123456789101112131415161718192021222324252627282930313233343536/** * A node inserted at head of bins during transfer operations. */static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125;&#125; 这个静态内部内就显得独具匠心，它的使用主要是在扩容阶段，它是链接两个table的节点类，有一个next属性用于指向下一个table，注意要理解这个table，它并不是说有2个table，而是在扩容的时候当线程读取到这个地方发现这个地方为空，这会设置为forwordingNode，或者线程处理完该节点也会设置该节点为forwordingNode，别的线程发现这个forwordingNode会继续向后执行遍历，这样一来就很好的解决了多线程安全的问题。这里有小伙伴就会问，那一个线程开始处理这个节点还没处理完，别的线程进来怎么办，而且这个节点还不是forwordingNode呐？说明你前面没看详细，在处理某个节点（桶里面第一个节点）的时候会对该节点上锁，上面文章中我已经说过了。 认识阶段就写到这里，对这些东西有一定的了解，在下一篇，也就是尾篇中，我会逐字逐句来介绍transfer（）扩容，put（）添加和get（）查询三个方法。 引言transfer方法（扩容方法）再这之前，我大致描述一下扩容的过程：首先有且只能由一个线程构建一个nextTable，这个nextTable主要是扩容后的数组（容量已经扩大），然后把原table复制到nextTable中，这个过程可以多线程共同操作。但是一定要清楚，这个复制并不是简单的把原table的数据直接移动到nextTable中，而是需要有一定的规律和算法操控的（不然怎么把树转化为链表呢）。 再这之前，先简单说下复制的过程：数组中（桶中）总共分为3种存储情况：空，链表头，TreeBin头①遍历原来的数组（原table），如果数组中某个值为空，则直接放置一个forwordingNode（上篇博文介绍过）。②如果数组中某个值不为空，而是一个链表头结点，那么就对这个链表进行拆分为两个链表，存储到nextTable对应的两个位置。③如果数组中某个值不为空，而是一个TreeBin头结点，那么这个地方就存储的是红黑树的结构，这样一来，处理就会变得相对比较复杂，就需要先判断需不需要把树转换为链表，做完一系列的处理，然后把对应的结果存储在nextTable的对应两个位置。 在上一篇博文中介绍过，多个线程进行扩容操作的时候，会判断原table的值，如果这个值是forwordingNode就表示这个节点被处理过了，就直接继续往下找。接下来，我们针对源码逐字逐句介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //stride 主要和CPU相关 //主要是判断CPU处理的量，如果小于16则直接赋值16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating只能有一个线程进行构造nextTable，如果别的线程进入发现不为空就不用构造nextTable了 try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; //把新的数组变为原来的两倍，这里的n&lt;&lt;1就是向左移动一位，也就是乘以二。 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; //原先扩容大小 &#125; int nextn = nextTab.length; //构造一个ForwardingNode用于多线程之间的共同扩容情况 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; //遍历的确认标志 boolean finishing = false; // to ensure sweep before committing nextTab //遍历每个节点 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; //定义一个节点和一个节点状态判断标志fh while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; //下面就是一个CAS计算 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; //如果原table已经复制结束 if (finishing) &#123; nextTable = null; //可以看出在扩容的时候nextTable只是类似于一个temp用完会丢掉 table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //修改扩容后的阀值，应该是现在容量的0.75倍 return;//结束循环 &#125; //采用CAS算法更新SizeCtl。 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; //CAS算法获取某一个数组的节点，为空就设为forwordingNode else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //如果这个节点的hash值是MOVED，就表示这个节点是forwordingNode节点，就表示这个节点已经被处理过了，直接跳过 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; //对头节点进行加锁，禁止别的线程进入 synchronized (f) &#123; //CAS校验这个节点是否在table对应的i处 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //如果这个节点的确是链表节点 //把链表拆分成两个小列表并存储到nextTable对应的两个位置 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); //CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); //CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; &#125; //如果这个节点是红黑树 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; //如果拆分后的树的节点数量已经少于6个就需要重新转化为链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; //CAS存储在nextTable的i位置上 setTabAt(nextTab, i, ln); //CAS存储在nextTable的i+n位置上 setTabAt(nextTab, i + n, hn); //CAS在原table的i处设置forwordingNode节点，表示这个这个节点已经处理完毕 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; PUT方法再这之前，先简单说一下PUT的具体操作：①先传入一个k和v的键值对，不可为空（HashMap是可以为空的），如果为空就直接报错。②接着去判断table是否为空，如果为空就进入初始化阶段。③如果判断数组中某个指定的桶是空的，那就直接把键值对插入到这个桶中作为头节点，而且这个操作不用加锁。④如果这个要插入的桶中的hash值为-1，也就是MOVED状态（也就是这个节点是forwordingNode），那就是说明有线程正在进行扩容操作，那么当前线程就进入协助扩容阶段。⑤需要把数据插入到链表或者树中，如果这个节点是一个链表节点，那么就遍历这个链表，如果发现有相同的key值就更新value值，如果遍历完了都没有发现相同的key值，就需要在链表的尾部插入该数据。插入结束之后判断该链表节点个数是否大于8，如果大于就需要把链表转化为红黑树存储。⑥如果这个节点是一个红黑树节点，那就需要按照树的插入规则进行插入。⑦put结束之后，需要给map已存储的数量+1，在addCount方法中判断是否需要扩容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123;//key和value都不可为空，为空直接抛出错误 if (key == null || value == null) throw new NullPointerException(); //计算Hash值，确定数组下标，这个和HashMap是一样的，我再HashMap的第一篇有介绍过 int hash = spread(key.hashCode()); int binCount = 0; //进入无线循环，直到插入为止 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table为空或者容量为0就表示没有初始化 if (tab == null || (n = tab.length) == 0) tab = initTable();//初始化数组 //CAS如果查询数组的某个桶是空的，就直接插入该桶中 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin这句话的意思是这个时候插入不用加锁 &#125; //如果在插入的时候，节点是一个forwordingNode状态，表示正在扩容，那么当前线程进行帮助扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; //锁定头节点 synchronized (f) &#123; //确定这个节点的确是数组中的这个头结点 if (tabAt(tab, i) == f) &#123; //是个链表 if (fh &gt;= 0) &#123; binCount = 1; //遍历这个链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果遍历到一个值，这个值和当前的key是相同的，那就更改value值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果遍历到结束都没有遇到相同的key，且后面没有节点了，那就直接在尾部插入一个 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果是红黑树存储就需要用红黑树的专门处理了，笔者不再展开。 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //判断节点数量是否大于8，如果大于就需要把链表转化成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //map已存储的数量+1 addCount(1L, binCount); return null;&#125; 其实，相对于transfer来说，PUT理解起来是不是简单很多？说到transfer，咋在PUT方法中都没出现过，只有一个helpTransfer（协助扩容）方法呢？其实，transfer方法放在了addCount方法中，下面是addCount方法的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //是否需要进行扩容操作 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); //如果小于0就说明已经再扩容或者已经在初始化 if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //如果是正在扩容就协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //如果正在初始化就首次发起扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; GET方法Get方法不论是在HashMap和ConcurrentHashMap都是最容易理解的，它的主要步骤是：①先判断数组的桶中的第一个节点是否寻找的对象是为链表还是红黑树，②如果是红黑树另外做处理③如果是链表就先判断头节点是否为要查找的节点，如果不是那么就遍历这个链表查询④如果都不是，那就返回null值。 1234567891011121314151617181920212223242526public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); //数组已被初始化且指定桶中不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //先判断头节点，如果头节点的hash值与入参key的hash值相同 if ((eh = e.hash) == h) &#123; //头节点的key就是传入的key if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val;//返回头节点的value值 &#125; //eh&lt;0表示这个节点是红黑树 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null;//直接从树上进行查找返回结果，不存在就返回null //如果首节点不是查找对象且不是红黑树结构，那边就遍历这个列表 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; //都没有找到就直接返回null值 return null;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>乐观锁</tag>
        <tag>悲观锁</tag>
        <tag>CAS算法</tag>
        <tag>volatile</tag>
        <tag>线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——HashMap]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94HashMap%2F</url>
    <content type="text"><![CDATA[HashMap底层1.数组和链表 数组 存储空间连续，占用内存严重，连续的大内存进入老年代的可能性也会变大，但是正因如此，寻址就显得简单，但是增删时则需要把数据整体往前或往后移动。 链表 存储空间不连续，占用内存较宽松，它的基本结构是一个节点（node）都会包含下一个节点的信息（如果双向链表会存在两个信息一个指向上一个，一个指向下一个），正因为如此寻址就会变得比较困难，插入和删除就显得容易，链表插入和删除的时候只需要修改节点指向信息就可以了。 2.哈希表/散列表（Hash Table非线程安全） 在哈希表的结构中就融入了数组和链表的结构，从而产生了一种寻址容易，插入删除也容易的新存储结构 为了解释方便，我们定义两个东西：String[] arr; 和 List list； 那么，上图左边那一列就是arr, 就是整个的arr[0]~arr[15]，且arr.length() = 16。上图每一行就是一个list，这里理论来说应该最大存储16个List。每个数组存存放的应该是某一个链表的头，也就是arr[0] == list.get(0)。不知道我这样的描述是否清楚。 那么如何确定某一个对象是属于数组的某个下标呢？一般算法就是 下标 = hash(key)%length。算式中的key是存放的对象，hash这个对象会得到一个int值，这个int值就是在上图中所体现的数字，length就是这个数组的长度。我们用上图中的arr[1]打个比方，1%16 =1，337%16 = 1， 353%16 =1。大家就存储在arr[1]中。 HashMap详解1.主要成员变量和方法 loadFactor：称为装载因子，主要控制空间利用率和冲突。大致记住装载因子越大空间利用率更高，但是冲突可能也会变大，反之则相反。源码中默认0.75f。 1234/** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; DEFAULT_INITIAL_CAPACITY与MAXIMUM_CAPACITY：称为容量，用于控制HashMap大小的，下面是源码中的解释： 1234567891011/** * The default initial初始 capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * The maximum capacity, used if a higher value is implicitly隐式 specified指定 * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; THRESHOLD: 这个字段主要是用于当HashMap的size大于它的时候，需要触发resize()方法进行扩容。下面是源码： 12345678910111213141516/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage.收缩 */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection发现 under removal移动. */static final int UNTREEIFY_THRESHOLD = 6; 看到这里，也许一部分人心中会有一个疑问，为什么前面赋值要用移位的方式，而这里就直接赋值8而不用1&lt;&lt;3呢？注意一个点，在上面有一句解释：“ MUST be a power of two.”，意思就是说这个变量如果发生变化将会是以2的幂次扩容的。比如说1&lt;&lt;4进行扩容一位的话就是1&lt;&lt;4&lt;1那结果就是32啦。 移位算法： 左移位：（低位补0） 例如：5&lt;&lt;2,表示5左移两位 101→10100 右移位：高位负数补1，正数补0 无符号右移：无论正负高位都补0 无论怎么移动，如果移动位数超过规定的bit数，都会与最大移位数取模之后进行计算 例如：int类型是32位的，如果5&lt;&lt;33,其实等价于5&lt;&lt;1 2.红黑树红黑树是自平衡的二叉查找树(有序二叉树)，一般的二叉查找树的时间复杂度为O(lgn)，当一般的二叉查找树退化之后会变成一个线性的列表，这个时候它的时间复杂度就变为了O(n)(这个O(n)其实就是for循环/遍历一次)，但是红黑树它所独有的着色和自平衡的一些性质使得时间复杂度最坏为O(logn)，而不是更低的O(n)，这就是等下我们要说的为什么HashMap在JDK1.8的时候引入冲突解决方案要用红黑树。 从图中就可以看到：①对于任意一个节点，它的左子节点比它小，右子节点比它大， 其实它还有更多的性质。我们就不一一研究了。我们只需要知道这一条性质和红黑树的优势就是可以自平衡。注意，本博客中提到的树性质就是①。 节点插入:遍历树，根据上面描述的性质，只要根据key值的大小可以很快定位到新要插入的节点位置。如果插入破坏了红黑树本身的平衡，红黑树会进行旋转，重新着色进行调整。 节点删除：分为3种情况。①第一要删除的节点处于最外层，那么就可以直接删除。②如果它存在一个子节点，这个子节点直接顶替要删除的节点后并不会破坏整棵树的性质，③如果它存在两个子节点，就先需要拿后继节点来顶替它的位置，在把该节点删除。那小伙伴说什么是后继节点？就上图而言，根节点13的后继节点就是11和15，也就是说节点左边最靠右的，和右边最靠左的。我这么说不知道能否理解呐？删除之后也可能会重新调整树本身的平衡。 3、&amp;与%：因为在HashMap中并不会用%来表达取模运算，而是要用&amp;来运算。也就是一定要记住如下等式： A%B = A&amp;(B-1)，源码中计算桶的位置都是用&amp;来计算的，记住这个，看源码就会轻松一些。 4.红黑树源码12345678910static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; //父节点 TreeNode&lt;K,V&gt; left;//左子节点 TreeNode&lt;K,V&gt; right;//右子节点 TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; //节点颜色 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; 5.核心hashMap123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178//reszie()方法： final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; //定义旧表并把原本的赋值给他，从昨天的博文中可以知道new了一个HashMap对象之后其实table是为null的。 int oldCap = (oldTab == null) ? 0 : oldTab.length;//如果旧表容量为null就初始0 int oldThr = threshold;//旧表的阀值 int newCap, newThr = 0;//定义新表的容量和新表的阀值 //进入条件：正常扩容 if (oldCap &gt; 0) &#123;如果旧表容量大于0，这个情况就是要扩容了 //进入条件：已达到最大，无法扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123;//如果容量已经大于等于1&lt;&lt;30 threshold = Integer.MAX_VALUE;//设置阀值最大 return oldTab;//直接返回原本的对象(无法扩大了) &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY)//旧表容量左移一位&lt;&lt;1,且移动之后处于合法的范围之中。新表容量扩充完成 newThr = oldThr &lt;&lt; 1; // 新表的阀值也扩大一倍。 &#125; //进入条件：初始化的时候使用了自定义加载因子的构造函数 else if (oldThr &gt; 0) // 这里如果执行的情况是原表容量为0的时候，但是阀值又不为0。hashmap的构造函数不同（需要设置自己的加载因子）的时候会触发。 newCap = oldThr; //进入条件：调用无参或者一个参数的构造函数进入默认初始化 else &#123; // 如果HashMap默认构造就会进入下面这个初始化，我们昨天（上一篇博文）的第一次put就会进入下面这一块。 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//初始化完成。 &#125; //进入条件：初始化的时候使用了自定义加载因子的构造函数 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor;//新表容量*加载因子 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;//确定新的阀值 //开始构造新表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //进入条件：原表存在 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123;//开始遍历 oldTab[j] = null;//旧表原本置空 if (e.next == null)//不存在下个节点，也就是目前put的就是链表头 newTab[e.hash &amp; (newCap - 1)] = e;把该对象赋值给新表的某一个桶中 //进入条件：判断桶中是否已红黑树存储的。如果是红黑树存储需要宁做判断 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //进入条件：如果桶中的值是合法的，也就是不止存在一个，也没有触发红黑树存储 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next;//获取下一个对象信息 //因为桶已经扩容了两倍，所以以下部分是按一定逻辑的把一个链表拆分为两个链表，放入对应桶中。具体的拆分流程，各位看官们仔细研究下。笔者已经看得头晕了。 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125;//put方法： public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125;//putVal()方法： final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //进入条件:HashMap初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //进入条件:如果计算得到的桶中不存在别的对象 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null);//直接把目前对象赋值给当前空对象中 else &#123; Node&lt;K,V&gt; e; K k; //进入条件：判断桶中第一个是否有相同的key值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //进入条件：如果该桶中的对象已经由红黑树构造了就特别处理 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; //进入条件：判断当前桶中对象存储是否达到8个，如果达到了就进入treeifyBin方法，这里不再进入方法详解。大致就是进入之后再判断当前hashMap的长度，如果不足64，只进行扩容，如果达到64就需要构建红黑树了。 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //进入条件：遍历中查看是否有相同的key值。如果有直接结束遍历并赋值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //进入条件：如果链表上有相同的key值。进行替换 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null)//这句笔者理解半天也未有所获，如果谁能清楚知道可以联系笔者一起探讨 e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; //如果hashMap的大小大于阀值就需要扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125;//get()方法： public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125;getNode()方法： final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //进入条件：如果HashMap不为空，且求得的数组下标那个对象不为空（关于%和&amp;的关系上面方法已有讲过） if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //先判断头结点是否是要找的值 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //然后再进入链表遍历 if ((e = first.next) != null) &#123; if (first instanceof TreeNode)//如果头链表已经是红黑树构造就需要用红黑树的方法去遍历 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; //直到找到拥有相同key的时候返回 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>数组</tag>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构——堆]]></title>
    <url>%2F2018%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%A0%86%2F</url>
    <content type="text"><![CDATA[引言堆，我们一般作为二叉堆的一种总称，它是建立在二叉树之上的。在本篇中，会详细介绍堆的结构和原理，以至于写出堆的实现。在代码实现中我们主要是针对于插入和删除做一些操作，在删除中我们只考虑删除最小的，而不涉及更深一步的操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Example&lt;T&gt; &#123; //定义一个头节点 Node head=null; //定义一个内部类，表示一个链表的节点 class Node&#123; private T t; Node next; public Node(T t)&#123; this.t=t; &#125; &#125; //链表插入数据 public void insert(T t)&#123; Node node = new Node(t);//node address=a1,t=1;node address=a2,t=2;node address=a3,t=3 //如果头结点是空，那就是首次插入 if(head==null)&#123; head=node;//head=node=a1 &#125;else &#123; //如果头结点不为空，那么寻找最后为空的位置插入 Node p=head;//p=a1;p=a1 while(p.next!=null)&#123; p=p.next;//p=a2 &#125; p.next=node;//p.next=a2;p.next=a3 &#125; &#125; //展示链表状态 public void print()&#123; Node p=head;//p=a1 while(p!=null)&#123; System.out.print(p.t+"-&gt;"); p=p.next; &#125; System.out.println("null\n"); &#125; public static void main(String[] args) &#123; //构建一个链表 Example&lt;Integer&gt; example = new Example&lt;&gt;(); example.insert(1); example.insert(2); example.insert(3); example.insert(4); example.insert(5); example.print(); &#125;&#125;//1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;null 场景引入我们在考虑优先队列的时候会有这样的场景：比如说整个公司都用同一台打印机。一般说来会有队列实现，它遵循FIFO的规则，先提交打印任务的先打印，这无可厚非。但是在实际中，我们希望重要的文件先打印，比如说甲有50页不重要的文件和乙有2页重要文件，甲先提交，这种情况下，我们希望乙能够先打印。FIFO的规则显然不合适。继续讨论这个问题，如果我们用自定义的链表实现呢？这里可以分为两种情况：①如果链表是有序的，那么删除最小的元素的时间复杂度是O(1)，但是插入的时间复杂度就是O(N)。②如果链表是无序的，那么插入定义为插入到最尾部，那么时间复杂度是O(1)，但是删除最小的元素时间复杂度就是O(N)。继续深究一下，如果我用二叉查找树呢？按照二叉查找树的性质来说，我们插入和删除最小元素的时间复杂度都是O(Log N)，相比于链表来说有一定的优化，但是我们要考虑一个问题，频繁的删除最小节点，会导致二叉查找树的退化，也就是说二叉查找树的右子树会比左子树大的多，也有可能会直接退化成链表。 完全二叉树通俗来说，在除最后一层外，每一层上的节点数均达到最大值，在最后一层上只缺少右边的若干结点。大家可以看下面这张图理解：再说明一下，只能缺少右边的若干节点，并不是可以缺少右子节点。 二叉堆堆是一颗被完全填满的二叉树，如果没有填满，那么只能是在最后一层，而且在这层上，所有的元素从左到右填入，如果有空缺，只能空缺右边的元素。通俗来说它就是一颗完全二叉树。同时它分为两类描述：①最小堆意思就是最小的元素在堆顶，且每一个父节点的值都要小于子节点，下图就是一个最小堆：②最大堆意思就是最大的在堆顶，且每一个父节点的值都大于子节点，下图就是一个最大堆：我们在代码实现过程中，已最小堆为例。 代码实现1、描述方式我们思考二叉堆，发现他不需要用链表来表述，直接用数组就可以表述了，我们尝试把二叉堆从上至下，一层一层平铺成数组。我们把上面的最小堆用数组表示就是： 我们对于其进行描述，对于一个二叉堆平铺之后的数组，我们可以发现，任意一个下标元素arr[i]，他的左孩子的就是arr[2i]，他的右孩子就是arr[2i+1]，他的父节点就是arr[i/2]。 为什么可以用数组来表述二叉堆？因为完全二叉树的性质，只能在最后一层的右侧允许缺少节点，而这些节点在数组中处于连续的末端，并不影响前面的所有元素。 2、插入二叉堆的插入还是很有意思的，一般，我们采用上滤的方式来解决二叉堆的插人：①确认一个可以插入的预插入位置，如果最后一层不满的话，那就插入到最后一层最靠左的那个位置，如果最后一层已满的话，那就新开一层，插入到最左边；②判断把当前数据放入到这个位置是否会破坏二叉堆的性质，如果不破坏，那么插入结束；③如果不符合，那么就需要把这个预插入位置和它的父节点进行兑换；重复②③步骤，直至插入结束。下面这张图描述了这种插入过程： 3、删除理解了插入的过程，删除其实也不难的。想对应的，我们称这种方法为下滤。在最小堆中，我们知道如果要删除最小的，那么其实就是删除堆顶就可以了。可想而知，那我们删除之后，有必要把整个二叉堆恢复到满足的条件。也就是说：①移除堆顶元素。并指定当前位置为预插入位置，并尝试把最后一个元素（最后一个元素在二叉堆的最后一层的最后一个位置）放到这个②如果不能顺利插入，那么就比较它的孩子，把较小的孩子放入这个预插入位置。③继续处理这个预插入位置，循环②步骤，直至又形成一个完整的二叉堆位置。下面这张图描述了这种删除最小的过程： 代码实现以下是用代码实现的二叉堆，包含了初始化，插入和删除： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139public class Heap&lt;T extends Comparable&lt;? super T&gt;&gt; &#123; private static final int DEFAULT_CAPACITY = 10; //默认容量 private T[] table; //用数组存储二叉堆 private int size; //表示当前二叉堆中有多少数据 public Heap(int capactiy)&#123; this.size = 0;//初始化二叉堆数据量 table = (T[]) new Comparable[capactiy + 1];//+1是因为我们要空出下标为0的元素不存储 &#125; public Heap() &#123;//显得专业些，你就要定义好构造器 this(DEFAULT_CAPACITY); &#125; //插入 public void insert(T t)&#123; //先判断是否需要扩容 if(size == table.length - 1)&#123; resize(); &#125; //开始插入 //定义一个预插入位置下标 int target = ++size; //循环比较父节点进行位置交换 for(table[ 0 ] = t; t.compareTo(table[target/2]) &lt; 0; target /= 2)&#123; table[target] = table[target/2];//如果满足条件，那么两者交换，直到找到合适位置（上滤） &#125; //插入数据 table[target] = t; print(); &#125; //删除最小 //删除过程中，需要重新调整二叉堆（下滤） public void deleteMin()&#123; if(size == 0)&#123; throw new IllegalAccessError("二叉堆为空"); &#125; //删除元素 table[1] = table[size--]; int target = 1;//从顶部开始重新调整二叉堆 int child;//要处理的节点下标 T tmp = table[ target ]; for( ; target * 2 &lt;= size; target = child ) &#123; child = target * 2; if( child != size &amp;&amp;table[ child + 1 ].compareTo( table[ child ] ) &lt; 0 )&#123;//如果右孩子比左孩子小 child++; &#125; if( table[ child ].compareTo( tmp ) &lt; 0 )&#123; table[ target ] = table[ child ]; table[child] = null; &#125; else&#123; break; &#125; &#125; table[ target ] = tmp; print(); &#125; //如果插入数据导致达到数组上限，那么就需要扩容 private void resize()&#123; T [] old = table; table = (T []) new Comparable[old.length*2 + 1];//把原来的数组扩大两倍 for( int i = 0; i &lt; old.length; i++ ) table[ i ] = old[ i ]; //数组进行拷贝 &#125; //打印数组 private void print()&#123; System.out.println(); for (int i = 1; i &lt;= size; i++) &#123; System.out.print(table[i] + " "); &#125; System.out.println("二叉堆大小:"+size); &#125; public static void main(String[] args) &#123; Heap&lt;Integer&gt; heap = new Heap&lt;&gt;(); //循环插入0~9的数据 for (int i = 0; i &lt; 10; i++) &#123; heap.insert(i); &#125; //循环删除3次，理论上是删除0,1,2 for (int i = 0; i &lt; 3; i++) &#123; heap.deleteMin(); &#125; &#125;&#125;//输出结果：////0 二叉堆大小:1////0 1 二叉堆大小:2////0 1 2 二叉堆大小:3////0 1 2 3 二叉堆大小:4////0 1 2 3 4 二叉堆大小:5////0 1 2 3 4 5 二叉堆大小:6////0 1 2 3 4 5 6 二叉堆大小:7////0 1 2 3 4 5 6 7 二叉堆大小:8////0 1 2 3 4 5 6 7 8 二叉堆大小:9////0 1 2 3 4 5 6 7 8 9 二叉堆大小:10////1 3 2 7 4 5 6 9 8 二叉堆大小:9////2 3 5 7 4 8 6 9 二叉堆大小:8////3 4 5 7 9 8 6 二叉堆大小:7 尾记这里，对于新手来说有一个小小的规则。对于一个类，代码模块存放顺序一般都是：静态成员变量/常量，构造方法，public方法，private方法。我主要说的是要把你封装起来的private方法放到最后面，因为别人查看你的代码的时候，别人希望最先看到的是你暴露出来的public方法，而不是对他来说无关紧要的private方法。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>堆</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
</search>
